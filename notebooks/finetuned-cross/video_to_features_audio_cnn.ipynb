{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm-A1pWMR3oa",
        "outputId": "c83dfc0d-13ed-45d7-898b-8b466b846795"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install facenet-pytorch\n",
        "!python3 -m pip install face_alignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNr_mImAjLT7",
        "outputId": "4f992506-f628-4af4-dc79-a2a2fed01f0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.16.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision->facenet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision->facenet-pytorch) (1.3.0)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.3\n",
            "Collecting face_alignment\n",
            "  Downloading face_alignment-1.4.1-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from face_alignment) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.11.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.8.0.76)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.66.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->face_alignment) (0.41.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->face_alignment) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->face_alignment) (1.3.0)\n",
            "Installing collected packages: face_alignment\n",
            "Successfully installed face_alignment-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SP57JsruRqX5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from facenet_pytorch import MTCNN\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import face_alignment\n",
        "import dlib\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GoogLeNetModified(models.GoogLeNet):\n",
        "    def _transform_input(self, x):\n",
        "        if x.size(1) == 1:\n",
        "            # If the input has only one channel, replicate it to create three channels\n",
        "            x = x.expand(-1, 3, -1, -1)\n",
        "\n",
        "        x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "        x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "        x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "        x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
        "\n",
        "        return x\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = GoogLeNetModified()\n",
        "        # Modify the first layer to accept 3 channel input (for RGB images)\n",
        "        self.features.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Modify the final layer to output the desired feature size\n",
        "        self.features.fc = nn.Linear(self.features.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Assuming the output is a tensor inside the GoogLeNetOutputs object\n",
        "        logits_tensor = x.logits if hasattr(x, 'logits') else x  # Adjust accordingly based on the structure\n",
        "\n",
        "        # Apply softmax directly on the logits tensor\n",
        "        x_softmax = torch.nn.functional.softmax(logits_tensor, dim=1)\n",
        "        return x_softmax\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oLkWoD6pRrAb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    mid_frame_index = frame_count // 2  # Index of the frame in the middle of the video\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_index)\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        cap.release()\n",
        "        return frame\n",
        "    else:\n",
        "        cap.release()\n",
        "        return None"
      ],
      "metadata": {
        "id": "gWDrEH00iv6H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face(frame):\n",
        "    mtcnn = MTCNN()\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    if boxes is not None:\n",
        "        # Assuming only one face in the frame\n",
        "        box = boxes[0]\n",
        "        x1, y1, x2, y2 = box\n",
        "        # Crop the frame to the detected face\n",
        "        cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "        return cropped_frame\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "_eeDIIORixs-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def align_face(frame):\n",
        "#     fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.FACE_2D, flip_input=False)\n",
        "#     landmarks = fa.get_landmarks(frame)\n",
        "#     if landmarks is not None:\n",
        "#         aligned_face = fa.align(frame, landmarks)\n",
        "#         return aligned_face\n",
        "#     else:\n",
        "#         return None"
      ],
      "metadata": {
        "id": "uCrtKaI8j4Fk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dlib\n",
        "# import requests\n",
        "\n",
        "# # Function to download the pretrained shape predictor file if it doesn't exist\n",
        "# def download_shape_predictor_file(url, save_path):\n",
        "#     if not os.path.exists(save_path):\n",
        "#         print(\"Downloading pretrained shape predictor file...\")\n",
        "#         response = requests.get(url)\n",
        "#         with open(save_path, 'wb') as f:\n",
        "#             f.write(response.content)\n",
        "#         print(\"Download complete.\")\n",
        "\n",
        "# # Specify the URL of the pretrained shape predictor file\n",
        "# shape_predictor_url = \"https://github.com/davisking/dlib-models/raw/master/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# def align_face(frame):\n",
        "#     # Download the pretrained shape predictor file if it doesn't exist\n",
        "#     shape_predictor_path = os.path.abspath(\"shape_predictor_68_face_landmarks.dat\")\n",
        "#     download_shape_predictor_file(shape_predictor_url, shape_predictor_path)\n",
        "\n",
        "#     # Initialize face detector and shape predictor\n",
        "#     detector = dlib.get_frontal_face_detector()\n",
        "#     predictor = dlib.shape_predictor(shape_predictor_path)\n",
        "\n",
        "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "#     rects = detector(gray, 0)\n",
        "\n",
        "#     if len(rects) == 1:  # Assuming only one face in the frame\n",
        "#         landmarks = predictor(gray, rects[0])\n",
        "#         aligned_face = dlib.get_face_chip(frame, landmarks)\n",
        "#         return aligned_face\n",
        "#     else:\n",
        "#         return None\n"
      ],
      "metadata": {
        "id": "4uaUCtgwlJUr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import face_alignment\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Function to download the pretrained face alignment model if it doesn't exist\n",
        "def download_face_alignment_model(url, save_path):\n",
        "    if not os.path.exists(save_path):\n",
        "        print(\"Downloading pretrained face alignment model...\")\n",
        "        response = requests.get(url)\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "# Specify the URL of the pretrained face alignment model\n",
        "face_alignment_model_url = \"https://github.com/1adrianb/face-alignment-models/releases/download/2.0.1/2DFAN4-11f355bf06.pth.tar\"\n",
        "\n",
        "# Download the pretrained face alignment model if it doesn't exist\n",
        "face_alignment_model_path = os.path.abspath(\"2DFAN4-11f355bf06.pth.tar\")\n",
        "download_face_alignment_model(face_alignment_model_url, face_alignment_model_path)\n",
        "\n",
        "# Initialize face alignment model\n",
        "fa = face_alignment.FaceAlignment(2, flip_input=False,device='cpu')  # 2 corresponds to 2D landmarks\n",
        "\n",
        "def align_face(frame):\n",
        "    # Perform face alignment\n",
        "    aligned_faces = fa.get_landmarks(frame)\n",
        "    if aligned_faces is not None:\n",
        "        aligned_face = aligned_faces[0]  # Assuming only one face in the frame\n",
        "        return aligned_face\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "pWavua0jl3ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c5e32f-2ccf-495e-b608-9cf2dff969dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pretrained face alignment model...\n",
            "Download complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n",
            "100%|██████████| 85.7M/85.7M [00:57<00:00, 1.55MB/s]\n",
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/3DFAN4-4a694010b9.zip\" to /root/.cache/torch/hub/checkpoints/3DFAN4-4a694010b9.zip\n",
            "100%|██████████| 91.9M/91.9M [01:49<00:00, 880kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(frame):\n",
        "    # Convert the frame to a PIL Image\n",
        "    frame_pil = Image.fromarray(frame.astype('uint8'))\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    frame_pil = frame_pil.convert('L')\n",
        "\n",
        "    # Resize and normalize the frame\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale images, only 1 channel\n",
        "    ])\n",
        "    img_tensor = transform(frame_pil)\n",
        "    return img_tensor"
      ],
      "metadata": {
        "id": "o5gBfMkuitP0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(input_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n",
        "    for video_file in tqdm(video_files):\n",
        "        video_path = os.path.join(input_folder, video_file)\n",
        "        frame = extract_frame(video_path)\n",
        "        if frame is not None:\n",
        "            cropped_face = detect_face(frame)\n",
        "            if cropped_face is not None:\n",
        "                preprocessed_face = preprocess_image(cropped_face)\n",
        "                X.append(preprocessed_face)\n",
        "                label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
        "                if label == \"HAP\":\n",
        "                    y.append(0)\n",
        "                elif label == \"SAD\":\n",
        "                    y.append(1)\n",
        "                elif label == \"ANG\":\n",
        "                    y.append(2)\n",
        "            else:\n",
        "                print(f\"No face detected in {video_file}. Skipping.\")\n",
        "        else:\n",
        "            print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "RZPMwNWJi5p3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_dataset(input_folder):\n",
        "#     X = []\n",
        "#     y = []\n",
        "#     video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n",
        "#     for video_file in tqdm(video_files):\n",
        "#         video_path = os.path.join(input_folder, video_file)\n",
        "#         frame = extract_frame(video_path)\n",
        "#         # crop and align\n",
        "#         if frame is not None:\n",
        "#             cropped_face = detect_face(frame)\n",
        "#             if cropped_face is not None:\n",
        "#                 aligned_face = align_face(cropped_face)\n",
        "#                 if aligned_face is not None:\n",
        "#                     preprocessed_face = preprocess_image(aligned_face)\n",
        "#                     X.append(preprocessed_face)\n",
        "#                     label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
        "#                     if label == \"HAP\":\n",
        "#                         y.append(0)\n",
        "#                     elif label == \"SAD\":\n",
        "#                         y.append(1)\n",
        "#                     elif label == \"ANG\":\n",
        "#                         y.append(2)\n",
        "#                 else:\n",
        "#                     print(f\"Failed to align face in {video_file}. Skipping.\")\n",
        "#             else:\n",
        "#                 print(f\"No face detected in {video_file}. Skipping.\")\n",
        "#         else:\n",
        "#             print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
        "#     return X, y\n"
      ],
      "metadata": {
        "id": "ASe2CY6_kJPn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_average_frames(video_path):\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "#     interval_frames = int(fps) # Frames to skip to get 1 frame per second\n",
        "#     frames = []\n",
        "#     for i in range(0, frame_count, interval_frames):\n",
        "#         cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "#         ret, frame = cap.read()\n",
        "#         if ret:\n",
        "#             frames.append(frame)\n",
        "#     cap.release()\n",
        "#     # Calculate average frame\n",
        "#     averaged_frame = sum(frames) / len(frames)\n",
        "#     return averaged_frame"
      ],
      "metadata": {
        "id": "OegULYZ0zV_R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_image(averaged_frame):\n",
        "#     # Convert the NumPy array to a PIL Image\n",
        "#     averaged_frame_pil = Image.fromarray(averaged_frame.astype('uint8'))\n",
        "\n",
        "#     # Convert the image to grayscale\n",
        "#     averaged_frame_pil = averaged_frame_pil.convert('L')\n",
        "\n",
        "#     # Resize and normalize the averaged frame\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.Resize((224, 224)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale images, only 1 channel\n",
        "#     ])\n",
        "#     img_tensor = transform(averaged_frame_pil)\n",
        "#     return img_tensor\n"
      ],
      "metadata": {
        "id": "YYjikmeiRsNP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_dataset(input_folder):\n",
        "#     X = []\n",
        "#     y = []\n",
        "#     # List all video files in the input folder\n",
        "#     video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n",
        "#     for video_file in video_files:\n",
        "#         video_path = os.path.join(input_folder, video_file)\n",
        "#         averaged_frame = extract_average_frames(video_path)\n",
        "#         img_tensor = preprocess_image(averaged_frame)\n",
        "#         X.append(img_tensor)\n",
        "#         # Extract label from video filename (assuming filename is in format \"label_videoID.mp4\")\n",
        "#         label = video_file.split(\"_\")[2]\n",
        "#         if label == \"HAP\":\n",
        "#             y.append(0)\n",
        "#         elif label == \"SAD\":\n",
        "#             y.append(1)\n",
        "#         elif label == \"ANG\":\n",
        "#             y.append(2)\n",
        "#         # print(video_path, label)\n",
        "#     return X, y"
      ],
      "metadata": {
        "id": "Y5pe1EXsSbpj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ],
      "metadata": {
        "id": "Q5--S5JwTTw1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ],
      "metadata": {
        "id": "ajpo8J6kTUjD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_features_from_folder(input_folder):\n",
        "#     # Initialize the model\n",
        "#     model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.to(device)\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "#     # List all files in the input folder\n",
        "#     files = os.listdir(input_folder)\n",
        "\n",
        "#     # Iterate over files in the folder\n",
        "#     for filename in files:\n",
        "#         if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n",
        "#             input_path = os.path.join(input_folder, filename)\n",
        "#             img_tensor = preprocess_image(input_path)\n",
        "#             img_tensor = img_tensor.to(device)\n",
        "#             with torch.no_grad():\n",
        "#                 output_features = model(img_tensor)\n",
        "#             print(f\"Features extracted for {filename}: {output_features}\")\n"
      ],
      "metadata": {
        "id": "92KI-5r7RtVi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_features_from_folder('/content/drive/MyDrive/csci535/melspec')"
      ],
      "metadata": {
        "id": "eBB1OzDcRxDy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 melspec_to_features_cnn.py /content/drive/MyDrive/csci535/melspec\n"
      ],
      "metadata": {
        "id": "mnnN_idgTBu5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if input arguments are provided\n",
        "    # if len(sys.argv) != 2:\n",
        "    #     print(\"Usage: python video_to_features_cnn.py input_folder\")\n",
        "    #     sys.exit(1)\n",
        "\n",
        "    # input_folder = sys.argv[1]\n",
        "    input_folder = '/content/drive/MyDrive/csci535/videos'\n",
        "    # Check if input folder exists\n",
        "    if not os.path.exists(input_folder):\n",
        "        print(\"Input folder does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load dataset and split into train and test sets\n",
        "    X, y = load_dataset(input_folder)\n",
        "    print(f\"Total number of samples: {len(X)}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    print(f\"Number of train samples: {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
        "    # Initialize the model\n",
        "    model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "    # Load the saved state dictionary\n",
        "    state_dict = torch.load('/content/drive/MyDrive/csci535/models/GoogLeNetModified_50_32_0.001')\n",
        "    # state_dict = torch.load('/content/drive/MyDrive/csci535/models/ResNet18_video_50_32_0.001')\n",
        "    # Load the state dictionary into the model\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    _lr = 0.001\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
        "\n",
        "    # Create data loaders\n",
        "    _bs = 32\n",
        "    # train_loader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=_bs, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(list(zip(X_test, y_test)), batch_size=_bs)\n",
        "    print(f\"Batch size: {_bs}\", f\"lr: {_lr}\")\n",
        "    # Training loop\n",
        "    # num_epochs = 50\n",
        "    # for epoch in range(num_epochs):\n",
        "        # print(\"Epoch \" + str(epoch))\n",
        "        # train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "        # test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "        # print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "    test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CtiJ8-bS7vS",
        "outputId": "2267e6b4-70b1-4c91-efd5-09d3e12b03b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 273/273 [00:26<00:00, 10.14it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 273\n",
            "Number of train samples: 191 Number of test samples: 82\n",
            "Batch size: 32 lr: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:07<00:00,  2.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.1606, Test Accuracy: 0.3902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), 'ResNet18_video_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))"
      ],
      "metadata": {
        "id": "9N8B8WXkUdJY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls -lh /content/"
      ],
      "metadata": {
        "id": "qdOvcYZea_DZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp  'ResNet18_video_50_32_0.001' '/content/drive/MyDrive/csci535/models'"
      ],
      "metadata": {
        "id": "VIsatL11bPU_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WYN9KsDs8lP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}