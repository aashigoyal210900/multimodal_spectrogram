{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm-A1pWMR3oa",
        "outputId": "3d693158-068d-466e-dcbf-c541fdb016be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install facenet-pytorch\n",
        "!python3 -m pip install face_alignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNr_mImAjLT7",
        "outputId": "7569d7a2-9603-4288-fad5-ee27fbdedf25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.16.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision->facenet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision->facenet-pytorch) (1.3.0)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.3\n",
            "Collecting face_alignment\n",
            "  Downloading face_alignment-1.4.1-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from face_alignment) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.11.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.8.0.76)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.66.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->face_alignment) (0.41.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->face_alignment) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->face_alignment) (1.3.0)\n",
            "Installing collected packages: face_alignment\n",
            "Successfully installed face_alignment-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SP57JsruRqX5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from facenet_pytorch import MTCNN\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import face_alignment\n",
        "import dlib\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GoogLeNetModified(models.GoogLeNet):\n",
        "    def _transform_input(self, x):\n",
        "        if x.size(1) == 1:\n",
        "            # If the input has only one channel, replicate it to create three channels\n",
        "            x = x.expand(-1, 3, -1, -1)\n",
        "\n",
        "        x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "        x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "        x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "        x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
        "\n",
        "        return x\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = GoogLeNetModified()\n",
        "        # Modify the first layer to accept 3 channel input (for RGB images)\n",
        "        self.features.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Modify the final layer to output the desired feature size\n",
        "        self.features.fc = nn.Linear(self.features.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Assuming the output is a tensor inside the GoogLeNetOutputs object\n",
        "        logits_tensor = x.logits if hasattr(x, 'logits') else x  # Adjust accordingly based on the structure\n",
        "\n",
        "        # Apply softmax directly on the logits tensor\n",
        "        x_softmax = torch.nn.functional.softmax(logits_tensor, dim=1)\n",
        "        return x_softmax\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oLkWoD6pRrAb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    mid_frame_index = frame_count // 2  # Index of the frame in the middle of the video\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_index)\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        cap.release()\n",
        "        return frame\n",
        "    else:\n",
        "        cap.release()\n",
        "        return None"
      ],
      "metadata": {
        "id": "gWDrEH00iv6H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face(frame):\n",
        "    mtcnn = MTCNN()\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    if boxes is not None:\n",
        "        # Assuming only one face in the frame\n",
        "        box = boxes[0]\n",
        "        x1, y1, x2, y2 = box\n",
        "        # Crop the frame to the detected face\n",
        "        cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "        return cropped_frame\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "_eeDIIORixs-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to download the pretrained face alignment model if it doesn't exist\n",
        "def download_face_alignment_model(url, save_path):\n",
        "    if not os.path.exists(save_path):\n",
        "        print(\"Downloading pretrained face alignment model...\")\n",
        "        response = requests.get(url)\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "# Specify the URL of the pretrained face alignment model\n",
        "face_alignment_model_url = \"https://github.com/1adrianb/face-alignment-models/releases/download/2.0.1/2DFAN4-11f355bf06.pth.tar\"\n",
        "\n",
        "# Download the pretrained face alignment model if it doesn't exist\n",
        "face_alignment_model_path = os.path.abspath(\"2DFAN4-11f355bf06.pth.tar\")\n",
        "download_face_alignment_model(face_alignment_model_url, face_alignment_model_path)\n",
        "\n",
        "# Initialize face alignment model\n",
        "fa = face_alignment.FaceAlignment(2, flip_input=False,device='cpu')  # 2 corresponds to 2D landmarks\n",
        "\n",
        "def align_face(frame):\n",
        "    # Perform face alignment\n",
        "    aligned_faces = fa.get_landmarks(frame)\n",
        "    if aligned_faces is not None:\n",
        "        aligned_face = aligned_faces[0]  # Assuming only one face in the frame\n",
        "        return aligned_face\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "pWavua0jl3ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "647e0ec6-fb5f-4d98-8aa8-77857651e70f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pretrained face alignment model...\n",
            "Download complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n",
            "100%|██████████| 85.7M/85.7M [00:04<00:00, 20.3MB/s]\n",
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/3DFAN4-4a694010b9.zip\" to /root/.cache/torch/hub/checkpoints/3DFAN4-4a694010b9.zip\n",
            "100%|██████████| 91.9M/91.9M [00:04<00:00, 19.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(frame):\n",
        "    # Convert the frame to a PIL Image\n",
        "    frame_pil = Image.fromarray(frame.astype('uint8'))\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    frame_pil = frame_pil.convert('L')\n",
        "\n",
        "    # Resize and normalize the frame\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale images, only 1 channel\n",
        "    ])\n",
        "    img_tensor = transform(frame_pil)\n",
        "    return img_tensor"
      ],
      "metadata": {
        "id": "o5gBfMkuitP0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_spectrogram(image_path):\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale images, only 1 channel\n",
        "    ])\n",
        "    img_tensor = transform(img)\n",
        "    # img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n",
        "    return img_tensor"
      ],
      "metadata": {
        "id": "x5iAGoZ_yUMX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_spectrogram_dataset(input_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    # List all files in the input folder\n",
        "    files = os.listdir(input_folder)\n",
        "    # Iterate over files in the folder\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            img_tensor = preprocess_spectrogram(input_path)\n",
        "            X.append(img_tensor)\n",
        "            # Extract label from filename (assuming filename is in format \"abc_IEO_label_xyz.png\")\n",
        "            label = filename.split(\"_\")[2]\n",
        "            if label == \"HAP\":\n",
        "                y.append(0)\n",
        "            elif label == \"SAD\":\n",
        "                y.append(1)\n",
        "            elif label == \"ANG\":\n",
        "                y.append(2)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "ONxd6uaQyYT8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(input_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n",
        "    for video_file in tqdm(video_files):\n",
        "        video_path = os.path.join(input_folder, video_file)\n",
        "        frame = extract_frame(video_path)\n",
        "        if frame is not None:\n",
        "            cropped_face = detect_face(frame)\n",
        "            if cropped_face is not None:\n",
        "                preprocessed_face = preprocess_image(cropped_face)\n",
        "                X.append(preprocessed_face)\n",
        "                label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
        "                if label == \"HAP\":\n",
        "                    y.append(0)\n",
        "                elif label == \"SAD\":\n",
        "                    y.append(1)\n",
        "                elif label == \"ANG\":\n",
        "                    y.append(2)\n",
        "            else:\n",
        "                print(f\"No face detected in {video_file}. Skipping.\")\n",
        "        else:\n",
        "            print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "RZPMwNWJi5p3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X1, X2, y):\n",
        "        self.X1 = X1\n",
        "        self.X2 = X2\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1 = self.X1[idx]\n",
        "        img2 = self.X2[idx]\n",
        "        label = self.y[idx]\n",
        "        # Concatenate images along the channel dimension\n",
        "        concatenated_img = torch.cat((img1, img2), dim=2)\n",
        "        return concatenated_img, label"
      ],
      "metadata": {
        "id": "O0bI2bJNzZ9r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ],
      "metadata": {
        "id": "Q5--S5JwTTw1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ],
      "metadata": {
        "id": "ajpo8J6kTUjD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = '/content/drive/MyDrive/csci535/videos'\n",
        "input_folder_spec = '/content/drive/MyDrive/csci535/melspec'\n",
        "\n",
        "# Check if input folder exists\n",
        "if not os.path.exists(input_folder):\n",
        "    print(\"Input folder does not exist.\")\n",
        "    sys.exit(1)\n",
        "# Check if input folder exists\n",
        "if not os.path.exists(input_folder_spec):\n",
        "    print(\"Input folder does not exist.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Load dataset and split into train and test sets\n",
        "X, y = load_dataset(input_folder)\n",
        "X_spec, y_spec = load_spectrogram_dataset(input_folder_spec)\n",
        "\n",
        "print(f\"Total number of samples: {len(X)}\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(f\"Number of train samples (video): {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
        "X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(X_spec, y_spec, test_size=0.3, random_state=42)\n",
        "print(f\"Number of train samples (audio): {len(X_train_spec)}\", f\"Number of test samples: {len(X_test_spec)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "D29Ahtfz2HAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55f2ad9-890d-4c1b-9ad9-6750b3f9119d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 273/273 [00:38<00:00,  7.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 273\n",
            "Number of train samples (video): 191 Number of test samples: 82\n",
            "Number of train samples (audio): 191 Number of test samples: 82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "_lr = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
        "\n",
        "# Concatenate datasets\n",
        "train_dataset = ConcatDataset(X_train, X_train_spec, y_train)\n",
        "test_dataset = ConcatDataset(X_test, X_test_spec, y_test)\n",
        "\n",
        "# Create data loaders\n",
        "_bs = 32\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=_bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_bs)\n",
        "\n",
        "\n",
        "print(f\"Batch size: {_bs}\", f\"lr: {_lr}\")"
      ],
      "metadata": {
        "id": "Q3xLVy9d2O5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e62a61-1e25-49f3-8190-241f630ede87"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 32 lr: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training loop\n",
        "# num_epochs = 50\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(\"Epoch \" + str(epoch))\n",
        "#     train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "#     test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "#num_epochs = 50\n",
        "#for epoch in range(num_epochs):\n",
        "#    train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "#    test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "#    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "#test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "#print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "c0upA86L2S47"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "_8gAw9Xi7ePC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8af358-a413-4a79-f2d6-e21b4bbcfcb6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.0987, Test Accuracy: 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), 'ResNet18_audio_video_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))"
      ],
      "metadata": {
        "id": "9N8B8WXkUdJY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls -lh /content/"
      ],
      "metadata": {
        "id": "qdOvcYZea_DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp  'ResNet18_audio_video_50_32_0.001' '/content/drive/MyDrive/csci535/models'"
      ],
      "metadata": {
        "id": "VIsatL11bPU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MkdnrPR5xd9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}