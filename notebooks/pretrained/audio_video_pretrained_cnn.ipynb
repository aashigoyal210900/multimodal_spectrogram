{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm-A1pWMR3oa",
        "outputId": "044c3ec0-cc4a-40e3-96cb-29a089d773b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNr_mImAjLT7",
        "outputId": "b078f379-6b53-47ca-b9d6-eae7a9612477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (2.5.3)\n",
            "Requirement already satisfied: numpy in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: requests in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from facenet-pytorch) (2.31.0)\n",
            "Requirement already satisfied: torchvision in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from facenet-pytorch) (0.17.1)\n",
            "Requirement already satisfied: pillow in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from requests->facenet-pytorch) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from requests->facenet-pytorch) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from requests->facenet-pytorch) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from requests->facenet-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: torch in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torchvision->facenet-pytorch) (2.2.1)\n",
            "Requirement already satisfied: filelock in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->torchvision->facenet-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->torchvision->facenet-pytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->torchvision->facenet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->torchvision->facenet-pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->torchvision->facenet-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->torchvision->facenet-pytorch) (2023.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from jinja2->torch->torchvision->facenet-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from sympy->torch->torchvision->facenet-pytorch) (1.3.0)\n",
            "Requirement already satisfied: face_alignment in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (1.4.1)\n",
            "Requirement already satisfied: torch in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from face_alignment) (2.2.1)\n",
            "Requirement already satisfied: numpy in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from face_alignment) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.17 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from face_alignment) (1.11.3)\n",
            "Requirement already satisfied: scikit-image in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from face_alignment) (0.22.0)\n",
            "Requirement already satisfied: opencv-python in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from face_alignment) (4.9.0.80)\n",
            "Requirement already satisfied: tqdm in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from face_alignment) (4.66.2)\n",
            "Requirement already satisfied: numba in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from face_alignment) (0.59.0)\n",
            "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from numba->face_alignment) (0.42.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from scikit-image->face_alignment) (3.1)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from scikit-image->face_alignment) (10.2.0)\n",
            "Requirement already satisfied: imageio>=2.27 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from scikit-image->face_alignment) (2.34.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from scikit-image->face_alignment) (2024.2.12)\n",
            "Requirement already satisfied: packaging>=21 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from scikit-image->face_alignment) (23.2)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from scikit-image->face_alignment) (0.3)\n",
            "Requirement already satisfied: filelock in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->face_alignment) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->face_alignment) (4.10.0)\n",
            "Requirement already satisfied: sympy in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->face_alignment) (1.12)\n",
            "Requirement already satisfied: jinja2 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->face_alignment) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from torch->face_alignment) (2023.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from jinja2->torch->face_alignment) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages (from sympy->torch->face_alignment) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install facenet-pytorch\n",
        "!python3 -m pip install face_alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SP57JsruRqX5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from facenet_pytorch import MTCNN\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import face_alignment\n",
        "import dlib\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oLkWoD6pRrAb"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = models.vgg16(pretrained=True)\n",
        "        # Modify the first layer to accept 1 channel input (for grayscale spectrograms)\n",
        "        self.features.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # Modify the final layer to output desired feature size\n",
        "        self.features.classifier[6] = nn.Linear(self.features.classifier[6].in_features, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gWDrEH00iv6H"
      },
      "outputs": [],
      "source": [
        "def extract_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    mid_frame_index = frame_count // 2  # Index of the frame in the middle of the video\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_index)\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        cap.release()\n",
        "        return frame\n",
        "    else:\n",
        "        cap.release()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_eeDIIORixs-"
      },
      "outputs": [],
      "source": [
        "def detect_face(frame):\n",
        "    mtcnn = MTCNN()\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    if boxes is not None:\n",
        "        # Assuming only one face in the frame\n",
        "        box = boxes[0]\n",
        "        x1, y1, x2, y2 = box\n",
        "        # Crop the frame to the detected face\n",
        "        cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "        return cropped_frame\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWavua0jl3ds",
        "outputId": "05697da0-b250-44d3-ecde-0d86f0a81bdb"
      },
      "outputs": [],
      "source": [
        "# Function to download the pretrained face alignment model if it doesn't exist\n",
        "def download_face_alignment_model(url, save_path):\n",
        "    if not os.path.exists(save_path):\n",
        "        print(\"Downloading pretrained face alignment model...\")\n",
        "        response = requests.get(url)\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete.\")\n",
        "device = 'cpu' \n",
        "# Specify the URL of the pretrained face alignment model\n",
        "face_alignment_model_url = \"https://github.com/1adrianb/face-alignment-models/releases/download/2.0.1/2DFAN4-11f355bf06.pth.tar\"\n",
        "\n",
        "# Download the pretrained face alignment model if it doesn't exist\n",
        "face_alignment_model_path = os.path.abspath(\"2DFAN4-11f355bf06.pth.tar\")\n",
        "download_face_alignment_model(face_alignment_model_url, face_alignment_model_path)\n",
        "\n",
        "# Initialize face alignment model\n",
        "fa = face_alignment.FaceAlignment(2, device=device,flip_input=False)  # 2 corresponds to 2D landmarks\n",
        "\n",
        "def align_face(frame):\n",
        "    # Perform face alignment\n",
        "    aligned_faces = fa.get_landmarks(frame)\n",
        "    if aligned_faces is not None:\n",
        "        aligned_face = aligned_faces[0]  # Assuming only one face in the frame\n",
        "        return aligned_face\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o5gBfMkuitP0"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(frame):\n",
        "    # Convert the frame to a PIL Image\n",
        "    frame_pil = Image.fromarray(frame.astype('uint8'))\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    frame_pil = frame_pil.convert('L')\n",
        "\n",
        "    # Resize and normalize the frame\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale images, only 1 channel\n",
        "    ])\n",
        "    img_tensor = transform(frame_pil)\n",
        "    return img_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x5iAGoZ_yUMX"
      },
      "outputs": [],
      "source": [
        "def preprocess_spectrogram(image_path):\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match ResNet input size\n",
        "        transforms.ToTensor(),           # Convert to tensor\n",
        "    ])\n",
        "    img_tensor = transform(img)\n",
        "    # img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n",
        "    return img_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ONxd6uaQyYT8"
      },
      "outputs": [],
      "source": [
        "def load_spectrogram_dataset(input_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    # List all files in the input folder\n",
        "    files = os.listdir(input_folder)\n",
        "    # Iterate over files in the folder\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            img_tensor = preprocess_spectrogram(input_path)\n",
        "            X.append(img_tensor)\n",
        "            # Extract label from filename (assuming filename is in format \"abc_IEO_label_xyz.png\")\n",
        "            label = filename.split(\"_\")[2]\n",
        "            if label == \"HAP\":\n",
        "                y.append(0)\n",
        "            elif label == \"SAD\":\n",
        "                y.append(1)\n",
        "            elif label == \"ANG\":\n",
        "                y.append(2)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RZPMwNWJi5p3"
      },
      "outputs": [],
      "source": [
        "def load_dataset(input_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n",
        "    for video_file in tqdm(video_files):\n",
        "        video_path = os.path.join(input_folder, video_file)\n",
        "        frame = extract_frame(video_path)\n",
        "        if frame is not None:\n",
        "            cropped_face = detect_face(frame)\n",
        "            if cropped_face is not None:\n",
        "                preprocessed_face = preprocess_image(cropped_face)\n",
        "                X.append(preprocessed_face)\n",
        "                label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
        "                if label == \"HAP\":\n",
        "                    y.append(0)\n",
        "                elif label == \"SAD\":\n",
        "                    y.append(1)\n",
        "                elif label == \"ANG\":\n",
        "                    y.append(2)\n",
        "            else:\n",
        "                print(f\"No face detected in {video_file}. Skipping.\")\n",
        "        else:\n",
        "            print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O0bI2bJNzZ9r"
      },
      "outputs": [],
      "source": [
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X1, X2, y):\n",
        "        self.X1 = X1\n",
        "        self.X2 = X2\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1 = self.X1[idx]\n",
        "        img2 = self.X2[idx]\n",
        "        label = self.y[idx]\n",
        "        # Concatenate images along the channel dimension\n",
        "        concatenated_img = torch.cat((img1, img2), dim=2)\n",
        "        return concatenated_img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q5--S5JwTTw1"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ajpo8J6kTUjD"
      },
      "outputs": [],
      "source": [
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D29Ahtfz2HAb",
        "outputId": "85170624-79c3-46ef-c185-28ea6b9b2146"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▋       | 72/273 [00:02<00:07, 27.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No face detected in 1083_IEO_HAP_HI.flv. Skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 273/273 [00:10<00:00, 26.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of samples: 272\n",
            "Number of train samples (video): 190 Number of test samples: 82\n",
            "Number of train samples (audio): 191 Number of test samples: 82\n"
          ]
        }
      ],
      "source": [
        "input_folder = '/Users/aashigoyal/Desktop/vscode/aashimain/csci535/multimodal_course_project/Datasets/videos'\n",
        "input_folder_spec = '/Users/aashigoyal/Desktop/vscode/aashimain/csci535/melspec'\n",
        "\n",
        "# Check if input folder exists\n",
        "if not os.path.exists(input_folder):\n",
        "    print(\"Input folder does not exist.\")\n",
        "    sys.exit(1)\n",
        "# Check if input folder exists\n",
        "if not os.path.exists(input_folder_spec):\n",
        "    print(\"Input folder does not exist.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Load dataset and split into train and test sets\n",
        "X, y = load_dataset(input_folder)\n",
        "X_spec, y_spec = load_spectrogram_dataset(input_folder_spec)\n",
        "\n",
        "print(f\"Total number of samples: {len(X)}\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(f\"Number of train samples (video): {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
        "X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(X_spec, y_spec, test_size=0.3, random_state=42)\n",
        "print(f\"Number of train samples (audio): {len(X_train_spec)}\", f\"Number of test samples: {len(X_test_spec)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3xLVy9d2O5d",
        "outputId": "e401ed65-54b6-49cf-fdae-efe9358d24e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 32 lr: 0.001\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "device = torch.device( \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "_lr = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
        "\n",
        "# Concatenate datasets\n",
        "# train_dataset = ConcatDataset(X_train, X_train_spec, y_train)\n",
        "test_dataset = ConcatDataset(X_test, X_test_spec, y_test)\n",
        "\n",
        "# Create data loaders\n",
        "_bs = 32\n",
        "\n",
        "# Create data loaders\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=_bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_bs)\n",
        "\n",
        "\n",
        "print(f\"Batch size: {_bs}\", f\"lr: {_lr}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "c0upA86L2S47"
      },
      "outputs": [],
      "source": [
        "# # Training loop\n",
        "# num_epochs = 50\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(\"Epoch \" + str(epoch))\n",
        "#     train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "#     test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8gAw9Xi7ePC",
        "outputId": "eb015fa1-febd-40f9-acfd-8208309d6bc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/aashigoyal/anaconda3/envs/csci/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 3/3 [00:10<00:00,  3.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.1029, Test Accuracy: 0.2683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9N8B8WXkUdJY"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), 'ResNet18_audio_video_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qdOvcYZea_DZ"
      },
      "outputs": [],
      "source": [
        "# !ls -lh /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VIsatL11bPU_"
      },
      "outputs": [],
      "source": [
        "# !cp  'ResNet18_audio_video_50_32_0.001' '/content/drive/MyDrive/csci535/models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2MkdnrPR5xd9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
