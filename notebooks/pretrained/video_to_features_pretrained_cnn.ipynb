{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm-A1pWMR3oa",
        "outputId": "ac44e0a3-00b4-402f-d86b-7aa675245954"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install facenet-pytorch\n",
        "!python3 -m pip install face_alignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNr_mImAjLT7",
        "outputId": "f6007935-b09b-4f78-8598-c3606c5ce260"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.16.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision->facenet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision->facenet-pytorch) (1.3.0)\n",
            "Requirement already satisfied: face_alignment in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from face_alignment) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.11.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.8.0.76)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.66.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->face_alignment) (0.41.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->face_alignment) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->face_alignment) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SP57JsruRqX5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from facenet_pytorch import MTCNN\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import face_alignment\n",
        "import dlib\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GoogLeNetModified(models.GoogLeNet):\n",
        "    def _transform_input(self, x):\n",
        "        if x.size(1) == 1:\n",
        "            # If the input has only one channel, replicate it to create three channels\n",
        "            x = x.expand(-1, 3, -1, -1)\n",
        "\n",
        "        x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "        x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "        x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "        x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
        "\n",
        "        return x\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = GoogLeNetModified()\n",
        "        # Modify the first layer to accept 3 channel input (for RGB images)\n",
        "        self.features.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Modify the final layer to output the desired feature size\n",
        "        self.features.fc = nn.Linear(self.features.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Assuming the output is a tensor inside the GoogLeNetOutputs object\n",
        "        logits_tensor = x.logits if hasattr(x, 'logits') else x  # Adjust accordingly based on the structure\n",
        "\n",
        "        # Apply softmax directly on the logits tensor\n",
        "        x_softmax = torch.nn.functional.softmax(logits_tensor, dim=1)\n",
        "        return x_softmax\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oLkWoD6pRrAb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    mid_frame_index = frame_count // 2  # Index of the frame in the middle of the video\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_index)\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        cap.release()\n",
        "        return frame\n",
        "    else:\n",
        "        cap.release()\n",
        "        return None"
      ],
      "metadata": {
        "id": "gWDrEH00iv6H"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face(frame):\n",
        "    mtcnn = MTCNN()\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    if boxes is not None:\n",
        "        # Assuming only one face in the frame\n",
        "        box = boxes[0]\n",
        "        x1, y1, x2, y2 = box\n",
        "        # Crop the frame to the detected face\n",
        "        cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "        return cropped_frame\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "_eeDIIORixs-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def align_face(frame):\n",
        "#     fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.FACE_2D, flip_input=False)\n",
        "#     landmarks = fa.get_landmarks(frame)\n",
        "#     if landmarks is not None:\n",
        "#         aligned_face = fa.align(frame, landmarks)\n",
        "#         return aligned_face\n",
        "#     else:\n",
        "#         return None"
      ],
      "metadata": {
        "id": "uCrtKaI8j4Fk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dlib\n",
        "# import requests\n",
        "\n",
        "# # Function to download the pretrained shape predictor file if it doesn't exist\n",
        "# def download_shape_predictor_file(url, save_path):\n",
        "#     if not os.path.exists(save_path):\n",
        "#         print(\"Downloading pretrained shape predictor file...\")\n",
        "#         response = requests.get(url)\n",
        "#         with open(save_path, 'wb') as f:\n",
        "#             f.write(response.content)\n",
        "#         print(\"Download complete.\")\n",
        "\n",
        "# # Specify the URL of the pretrained shape predictor file\n",
        "# shape_predictor_url = \"https://github.com/davisking/dlib-models/raw/master/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# def align_face(frame):\n",
        "#     # Download the pretrained shape predictor file if it doesn't exist\n",
        "#     shape_predictor_path = os.path.abspath(\"shape_predictor_68_face_landmarks.dat\")\n",
        "#     download_shape_predictor_file(shape_predictor_url, shape_predictor_path)\n",
        "\n",
        "#     # Initialize face detector and shape predictor\n",
        "#     detector = dlib.get_frontal_face_detector()\n",
        "#     predictor = dlib.shape_predictor(shape_predictor_path)\n",
        "\n",
        "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "#     rects = detector(gray, 0)\n",
        "\n",
        "#     if len(rects) == 1:  # Assuming only one face in the frame\n",
        "#         landmarks = predictor(gray, rects[0])\n",
        "#         aligned_face = dlib.get_face_chip(frame, landmarks)\n",
        "#         return aligned_face\n",
        "#     else:\n",
        "#         return None\n"
      ],
      "metadata": {
        "id": "4uaUCtgwlJUr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import face_alignment\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Function to download the pretrained face alignment model if it doesn't exist\n",
        "def download_face_alignment_model(url, save_path):\n",
        "    if not os.path.exists(save_path):\n",
        "        print(\"Downloading pretrained face alignment model...\")\n",
        "        response = requests.get(url)\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "# Specify the URL of the pretrained face alignment model\n",
        "face_alignment_model_url = \"https://github.com/1adrianb/face-alignment-models/releases/download/2.0.1/2DFAN4-11f355bf06.pth.tar\"\n",
        "\n",
        "# Download the pretrained face alignment model if it doesn't exist\n",
        "face_alignment_model_path = os.path.abspath(\"2DFAN4-11f355bf06.pth.tar\")\n",
        "download_face_alignment_model(face_alignment_model_url, face_alignment_model_path)\n",
        "\n",
        "# Initialize face alignment model\n",
        "fa = face_alignment.FaceAlignment(2, flip_input=False)  # 2 corresponds to 2D landmarks\n",
        "\n",
        "def align_face(frame):\n",
        "    # Perform face alignment\n",
        "    aligned_faces = fa.get_landmarks(frame)\n",
        "    if aligned_faces is not None:\n",
        "        aligned_face = aligned_faces[0]  # Assuming only one face in the frame\n",
        "        return aligned_face\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "pWavua0jl3ds"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(frame):\n",
        "    # Convert the frame to a PIL Image\n",
        "    frame_pil = Image.fromarray(frame.astype('uint8'))\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    frame_pil = frame_pil.convert('L')\n",
        "\n",
        "    # Resize and normalize the frame\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale images, only 1 channel\n",
        "    ])\n",
        "    img_tensor = transform(frame_pil)\n",
        "    return img_tensor"
      ],
      "metadata": {
        "id": "o5gBfMkuitP0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(input_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n",
        "    for video_file in tqdm(video_files):\n",
        "        video_path = os.path.join(input_folder, video_file)\n",
        "        frame = extract_frame(video_path)\n",
        "        if frame is not None:\n",
        "            cropped_face = detect_face(frame)\n",
        "            if cropped_face is not None:\n",
        "                preprocessed_face = preprocess_image(cropped_face)\n",
        "                X.append(preprocessed_face)\n",
        "                label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
        "                if label == \"HAP\":\n",
        "                    y.append(0)\n",
        "                elif label == \"SAD\":\n",
        "                    y.append(1)\n",
        "                elif label == \"ANG\":\n",
        "                    y.append(2)\n",
        "            else:\n",
        "                print(f\"No face detected in {video_file}. Skipping.\")\n",
        "        else:\n",
        "            print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "RZPMwNWJi5p3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_dataset(input_folder):\n",
        "#     X = []\n",
        "#     y = []\n",
        "#     video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n",
        "#     for video_file in tqdm(video_files):\n",
        "#         video_path = os.path.join(input_folder, video_file)\n",
        "#         frame = extract_frame(video_path)\n",
        "#         # crop and align\n",
        "#         if frame is not None:\n",
        "#             cropped_face = detect_face(frame)\n",
        "#             if cropped_face is not None:\n",
        "#                 aligned_face = align_face(cropped_face)\n",
        "#                 if aligned_face is not None:\n",
        "#                     preprocessed_face = preprocess_image(aligned_face)\n",
        "#                     X.append(preprocessed_face)\n",
        "#                     label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
        "#                     if label == \"HAP\":\n",
        "#                         y.append(0)\n",
        "#                     elif label == \"SAD\":\n",
        "#                         y.append(1)\n",
        "#                     elif label == \"ANG\":\n",
        "#                         y.append(2)\n",
        "#                 else:\n",
        "#                     print(f\"Failed to align face in {video_file}. Skipping.\")\n",
        "#             else:\n",
        "#                 print(f\"No face detected in {video_file}. Skipping.\")\n",
        "#         else:\n",
        "#             print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
        "#     return X, y\n"
      ],
      "metadata": {
        "id": "ASe2CY6_kJPn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_average_frames(video_path):\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "#     interval_frames = int(fps) # Frames to skip to get 1 frame per second\n",
        "#     frames = []\n",
        "#     for i in range(0, frame_count, interval_frames):\n",
        "#         cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "#         ret, frame = cap.read()\n",
        "#         if ret:\n",
        "#             frames.append(frame)\n",
        "#     cap.release()\n",
        "#     # Calculate average frame\n",
        "#     averaged_frame = sum(frames) / len(frames)\n",
        "#     return averaged_frame"
      ],
      "metadata": {
        "id": "OegULYZ0zV_R"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_image(averaged_frame):\n",
        "#     # Convert the NumPy array to a PIL Image\n",
        "#     averaged_frame_pil = Image.fromarray(averaged_frame.astype('uint8'))\n",
        "\n",
        "#     # Convert the image to grayscale\n",
        "#     averaged_frame_pil = averaged_frame_pil.convert('L')\n",
        "\n",
        "#     # Resize and normalize the averaged frame\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.Resize((224, 224)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale images, only 1 channel\n",
        "#     ])\n",
        "#     img_tensor = transform(averaged_frame_pil)\n",
        "#     return img_tensor\n"
      ],
      "metadata": {
        "id": "YYjikmeiRsNP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_dataset(input_folder):\n",
        "#     X = []\n",
        "#     y = []\n",
        "#     # List all video files in the input folder\n",
        "#     video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n",
        "#     for video_file in video_files:\n",
        "#         video_path = os.path.join(input_folder, video_file)\n",
        "#         averaged_frame = extract_average_frames(video_path)\n",
        "#         img_tensor = preprocess_image(averaged_frame)\n",
        "#         X.append(img_tensor)\n",
        "#         # Extract label from video filename (assuming filename is in format \"label_videoID.mp4\")\n",
        "#         label = video_file.split(\"_\")[2]\n",
        "#         if label == \"HAP\":\n",
        "#             y.append(0)\n",
        "#         elif label == \"SAD\":\n",
        "#             y.append(1)\n",
        "#         elif label == \"ANG\":\n",
        "#             y.append(2)\n",
        "#         # print(video_path, label)\n",
        "#     return X, y"
      ],
      "metadata": {
        "id": "Y5pe1EXsSbpj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ],
      "metadata": {
        "id": "Q5--S5JwTTw1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ],
      "metadata": {
        "id": "ajpo8J6kTUjD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_features_from_folder(input_folder):\n",
        "#     # Initialize the model\n",
        "#     model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.to(device)\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "#     # List all files in the input folder\n",
        "#     files = os.listdir(input_folder)\n",
        "\n",
        "#     # Iterate over files in the folder\n",
        "#     for filename in files:\n",
        "#         if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n",
        "#             input_path = os.path.join(input_folder, filename)\n",
        "#             img_tensor = preprocess_image(input_path)\n",
        "#             img_tensor = img_tensor.to(device)\n",
        "#             with torch.no_grad():\n",
        "#                 output_features = model(img_tensor)\n",
        "#             print(f\"Features extracted for {filename}: {output_features}\")\n"
      ],
      "metadata": {
        "id": "92KI-5r7RtVi"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_features_from_folder('/content/drive/MyDrive/csci535/melspec')"
      ],
      "metadata": {
        "id": "eBB1OzDcRxDy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 melspec_to_features_cnn.py /content/drive/MyDrive/csci535/melspec\n"
      ],
      "metadata": {
        "id": "mnnN_idgTBu5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if input arguments are provided\n",
        "    # if len(sys.argv) != 2:\n",
        "    #     print(\"Usage: python video_to_features_cnn.py input_folder\")\n",
        "    #     sys.exit(1)\n",
        "\n",
        "    # input_folder = sys.argv[1]\n",
        "    input_folder = '/content/drive/MyDrive/csci535/videos'\n",
        "    # Check if input folder exists\n",
        "    if not os.path.exists(input_folder):\n",
        "        print(\"Input folder does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load dataset and split into train and test sets\n",
        "    X, y = load_dataset(input_folder)\n",
        "    print(f\"Total number of samples: {len(X)}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    print(f\"Number of train samples: {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
        "    # Initialize the model\n",
        "    model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "    # Load the saved state dictionary\n",
        "    # state_dict = torch.load('/content/drive/MyDrive/csci535/models/ResNet18_melspec_50_32_0.001')\n",
        "    # state_dict = torch.load('/content/drive/MyDrive/csci535/models/ResNet18_video_50_32_0.001')\n",
        "    # Load the state dictionary into the model\n",
        "    # model.load_state_dict(state_dict)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    _lr = 0.001\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
        "\n",
        "    # Create data loaders\n",
        "    _bs = 32\n",
        "    train_loader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=_bs, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(list(zip(X_test, y_test)), batch_size=_bs)\n",
        "    print(f\"Batch size: {_bs}\", f\"lr: {_lr}\")\n",
        "    # Training loop\n",
        "    # num_epochs = 50\n",
        "    # for epoch in range(num_epochs):\n",
        "        # print(\"Epoch \" + str(epoch))\n",
        "        # train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "        # test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "        # print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "    # Training loop\n",
        "    num_epochs = 50\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "        test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CtiJ8-bS7vS",
        "outputId": "20b0cb14-0a1d-492d-bc2c-c1b275a2773e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 273/273 [00:50<00:00,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 273\n",
            "Number of train samples: 191 Number of test samples: 82\n",
            "Batch size: 32 lr: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  8.07it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 1.0842, Train Accuracy: 0.4503, Test Loss: 1.1333, Test Accuracy: 0.3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.08it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50, Train Loss: 1.0185, Train Accuracy: 0.4921, Test Loss: 1.1748, Test Accuracy: 0.3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.08it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50, Train Loss: 0.9636, Train Accuracy: 0.5864, Test Loss: 1.1968, Test Accuracy: 0.3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.22it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50, Train Loss: 0.9510, Train Accuracy: 0.5759, Test Loss: 1.2051, Test Accuracy: 0.3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.21it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Train Loss: 0.9093, Train Accuracy: 0.6387, Test Loss: 1.2091, Test Accuracy: 0.3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.25it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50, Train Loss: 0.9003, Train Accuracy: 0.6597, Test Loss: 1.1930, Test Accuracy: 0.3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.23it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50, Train Loss: 0.8687, Train Accuracy: 0.6806, Test Loss: 1.1417, Test Accuracy: 0.3902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.19it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 36.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50, Train Loss: 0.9071, Train Accuracy: 0.6230, Test Loss: 1.0109, Test Accuracy: 0.5366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.21it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50, Train Loss: 0.9083, Train Accuracy: 0.6387, Test Loss: 1.0106, Test Accuracy: 0.5244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.16it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 33.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Train Loss: 0.8791, Train Accuracy: 0.6597, Test Loss: 0.9411, Test Accuracy: 0.6098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.25it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50, Train Loss: 0.8989, Train Accuracy: 0.6335, Test Loss: 1.0001, Test Accuracy: 0.5366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.31it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50, Train Loss: 0.8830, Train Accuracy: 0.6649, Test Loss: 1.1418, Test Accuracy: 0.3780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.20it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50, Train Loss: 0.8464, Train Accuracy: 0.7016, Test Loss: 0.9030, Test Accuracy: 0.6463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.19it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50, Train Loss: 0.8587, Train Accuracy: 0.6806, Test Loss: 0.9135, Test Accuracy: 0.6341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.25it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50, Train Loss: 0.8641, Train Accuracy: 0.6702, Test Loss: 1.0018, Test Accuracy: 0.5244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.19it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50, Train Loss: 0.9034, Train Accuracy: 0.6387, Test Loss: 1.1182, Test Accuracy: 0.4024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.13it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50, Train Loss: 0.8627, Train Accuracy: 0.6806, Test Loss: 1.1106, Test Accuracy: 0.4390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.14it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50, Train Loss: 0.8676, Train Accuracy: 0.6859, Test Loss: 1.0454, Test Accuracy: 0.4878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.06it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50, Train Loss: 0.8232, Train Accuracy: 0.7277, Test Loss: 0.9601, Test Accuracy: 0.5732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.08it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50, Train Loss: 0.8453, Train Accuracy: 0.7068, Test Loss: 1.0497, Test Accuracy: 0.4756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.00it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50, Train Loss: 0.8350, Train Accuracy: 0.7173, Test Loss: 0.9672, Test Accuracy: 0.5854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.13it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50, Train Loss: 0.8240, Train Accuracy: 0.7225, Test Loss: 0.9769, Test Accuracy: 0.5610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.13it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50, Train Loss: 0.8812, Train Accuracy: 0.6754, Test Loss: 1.1082, Test Accuracy: 0.4390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.05it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50, Train Loss: 0.8627, Train Accuracy: 0.6859, Test Loss: 1.0688, Test Accuracy: 0.4756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.14it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50, Train Loss: 0.8621, Train Accuracy: 0.6911, Test Loss: 1.0963, Test Accuracy: 0.4390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.17it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50, Train Loss: 0.8288, Train Accuracy: 0.7277, Test Loss: 0.9717, Test Accuracy: 0.5732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.03it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50, Train Loss: 0.8182, Train Accuracy: 0.7382, Test Loss: 1.0175, Test Accuracy: 0.5244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.12it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50, Train Loss: 0.8349, Train Accuracy: 0.7173, Test Loss: 0.9606, Test Accuracy: 0.5854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.05it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50, Train Loss: 0.7938, Train Accuracy: 0.7487, Test Loss: 0.9373, Test Accuracy: 0.5976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.15it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50, Train Loss: 0.8370, Train Accuracy: 0.7120, Test Loss: 1.0445, Test Accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.03it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50, Train Loss: 0.8112, Train Accuracy: 0.7487, Test Loss: 1.0468, Test Accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.03it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50, Train Loss: 0.7910, Train Accuracy: 0.7435, Test Loss: 1.0687, Test Accuracy: 0.4634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.10it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 36.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50, Train Loss: 0.7735, Train Accuracy: 0.7749, Test Loss: 0.9050, Test Accuracy: 0.6463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.09it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50, Train Loss: 0.8034, Train Accuracy: 0.7487, Test Loss: 0.8605, Test Accuracy: 0.6951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.06it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50, Train Loss: 0.7946, Train Accuracy: 0.7435, Test Loss: 1.1267, Test Accuracy: 0.4268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  8.99it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50, Train Loss: 0.7951, Train Accuracy: 0.7592, Test Loss: 0.8505, Test Accuracy: 0.7195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.05it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 33.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50, Train Loss: 0.7567, Train Accuracy: 0.7958, Test Loss: 0.9138, Test Accuracy: 0.6341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.03it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50, Train Loss: 0.7611, Train Accuracy: 0.7906, Test Loss: 1.0258, Test Accuracy: 0.5122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.09it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50, Train Loss: 0.7506, Train Accuracy: 0.8010, Test Loss: 0.9918, Test Accuracy: 0.5488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.07it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50, Train Loss: 0.7387, Train Accuracy: 0.8220, Test Loss: 0.8347, Test Accuracy: 0.7317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.04it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50, Train Loss: 0.7713, Train Accuracy: 0.7801, Test Loss: 0.9313, Test Accuracy: 0.6220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.07it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50, Train Loss: 0.7923, Train Accuracy: 0.7592, Test Loss: 0.8630, Test Accuracy: 0.6951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.05it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50, Train Loss: 0.7594, Train Accuracy: 0.7906, Test Loss: 1.0782, Test Accuracy: 0.4634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.07it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50, Train Loss: 0.7924, Train Accuracy: 0.7592, Test Loss: 1.0728, Test Accuracy: 0.4756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.05it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50, Train Loss: 0.7818, Train Accuracy: 0.7696, Test Loss: 1.0241, Test Accuracy: 0.5244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.06it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50, Train Loss: 0.7999, Train Accuracy: 0.7382, Test Loss: 0.9351, Test Accuracy: 0.6098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.07it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 35.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50, Train Loss: 0.7574, Train Accuracy: 0.7958, Test Loss: 1.1103, Test Accuracy: 0.4390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.02it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50, Train Loss: 0.7569, Train Accuracy: 0.8063, Test Loss: 0.9969, Test Accuracy: 0.5488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.08it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 33.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50, Train Loss: 0.7449, Train Accuracy: 0.8063, Test Loss: 0.9524, Test Accuracy: 0.5976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  8.98it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 34.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50, Train Loss: 0.7098, Train Accuracy: 0.8377, Test Loss: 0.9439, Test Accuracy: 0.5854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 34.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9439, Test Accuracy: 0.5854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), 'ResNet18_video_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))"
      ],
      "metadata": {
        "id": "9N8B8WXkUdJY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls -lh /content/"
      ],
      "metadata": {
        "id": "qdOvcYZea_DZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp  'ResNet18_video_50_32_0.001' '/content/drive/MyDrive/csci535/models'"
      ],
      "metadata": {
        "id": "VIsatL11bPU_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WYN9KsDs8lP"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}