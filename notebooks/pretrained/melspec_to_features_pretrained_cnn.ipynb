{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXK8FEiunzOa",
        "outputId": "562acddf-f4fe-4003-8f8b-bfeeb8332007"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SP57JsruRqX5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import dlib\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZtC9ol4hatmQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GoogLeNetModified(models.GoogLeNet):\n",
        "    def _transform_input(self, x):\n",
        "        if x.size(1) == 1:\n",
        "            # If the input has only one channel, replicate it to create three channels\n",
        "            x = x.expand(-1, 3, -1, -1)\n",
        "\n",
        "        x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "        x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "        x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "        x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
        "\n",
        "        return x\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = GoogLeNetModified()\n",
        "        # Modify the first layer to accept 3 channel input (for RGB images)\n",
        "        self.features.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Modify the final layer to output the desired feature size\n",
        "        self.features.fc = nn.Linear(self.features.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Assuming the output is a tensor inside the GoogLeNetOutputs object\n",
        "        logits_tensor = x.logits if hasattr(x, 'logits') else x  # Adjust accordingly based on the structure\n",
        "\n",
        "        # Apply softmax directly on the logits tensor\n",
        "        x_softmax = torch.nn.functional.softmax(logits_tensor, dim=1)\n",
        "        return x_softmax\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oLkWoD6pRrAb"
      },
      "outputs": [],
      "source": [
        "#import torch.nn as nn\n",
        "#import torchvision.models as models\n",
        "#from torchvision.models import googlenet\n",
        "\n",
        "#class CNN(nn.Module):\n",
        " #   def __init__(self, num_classes):\n",
        "  #      super(CNN, self).__init__()\n",
        "  #      # Load the Inception GoogLeNet model\n",
        "   #     self.features = models.inception_v3(pretrained=True)\n",
        "\n",
        "   #     # Modify the input layer to accept 1 channel input (for grayscale spectrograms)\n",
        "   #     self.features.Conv2d_1a_3x3.conv = nn.Conv2d(1, 32, kernel_size=3, stride=2, bias=False)\n",
        "\n",
        "    #    # Modify the final fully connected layer to output the desired number of classes\n",
        "    #    in_features = self.features.fc.in_features\n",
        "    #    self.features.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    #    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    #def forward(self, x):\n",
        "    #    x = self.features(x)\n",
        "    #    x = self.softmax(x)\n",
        "    #    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YYjikmeiRsNP"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale images, only 1 channel\n",
        "    ])\n",
        "    img_tensor = transform(img)\n",
        "    # If model expects RGB, convert grayscale images to RGB before feeding them:\n",
        "\n",
        "\n",
        "    # img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n",
        "    return img_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y5pe1EXsSbpj"
      },
      "outputs": [],
      "source": [
        "def load_dataset(input_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    # List all files in the input folder\n",
        "    files = os.listdir(input_folder)\n",
        "    # Iterate over files in the folder\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            img_tensor = preprocess_image(input_path)\n",
        "            X.append(img_tensor)\n",
        "            # Extract label from filename (assuming filename is in format \"abc_IEO_label_xyz.png\")\n",
        "            label = filename.split(\"_\")[2]\n",
        "            if label == \"HAP\":\n",
        "                y.append(0)\n",
        "            elif label == \"SAD\":\n",
        "                y.append(1)\n",
        "            elif label == \"ANG\":\n",
        "                y.append(2)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q5--S5JwTTw1"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ajpo8J6kTUjD"
      },
      "outputs": [],
      "source": [
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "92KI-5r7RtVi"
      },
      "outputs": [],
      "source": [
        "# def extract_features_from_folder(input_folder):\n",
        "#     # Initialize the model\n",
        "#     model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.to(device)\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "#     # List all files in the input folder\n",
        "#     files = os.listdir(input_folder)\n",
        "\n",
        "#     # Iterate over files in the folder\n",
        "#     for filename in files:\n",
        "#         if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n",
        "#             input_path = os.path.join(input_folder, filename)\n",
        "#             img_tensor = preprocess_image(input_path)\n",
        "#             img_tensor = img_tensor.to(device)\n",
        "#             with torch.no_grad():\n",
        "#                 output_features = model(img_tensor)\n",
        "#             print(f\"Features extracted for {filename}: {output_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eBB1OzDcRxDy"
      },
      "outputs": [],
      "source": [
        "# extract_features_from_folder('/content/drive/MyDrive/csci535/melspec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mnnN_idgTBu5"
      },
      "outputs": [],
      "source": [
        "# !python3 melspec_to_features_cnn.py /content/drive/MyDrive/csci535/melspec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CtiJ8-bS7vS",
        "outputId": "0baa5bb2-5772-4f39-ce19-52aa48248f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 273\n",
            "Number of train samples: 191 Number of test samples: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 32 lr: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.0993, Test Accuracy: 0.2805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if input arguments are provided\n",
        "    # if len(sys.argv) != 2:\n",
        "    #     print(\"Usage: python melspec_to_features_cnn.py input_folder\")\n",
        "    #     sys.exit(1)\n",
        "\n",
        "    # input_folder = sys.argv[1]\n",
        "    input_folder = '/content/drive/MyDrive/csci535/melspec'\n",
        "    # Check if input folder exists\n",
        "    if not os.path.exists(input_folder):\n",
        "        print(\"Input folder does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load dataset and split into train and test sets\n",
        "    X, y = load_dataset(input_folder)\n",
        "    print(f\"Total number of samples: {len(X)}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    print(f\"Number of train samples: {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
        "    # Initialize the model\n",
        "    model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "    # Load the saved state dictionary\n",
        "    # state_dict = torch.load('/content/drive/MyDrive/csci535/models/ResNet18_melspec_50_32_0.001')\n",
        "    # state_dict = torch.load('/content/drive/MyDrive/csci535/models/ResNet18_video_50_32_0.001')\n",
        "    # Load the state dictionary into the model\n",
        "    # model.load_state_dict(state_dict)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    _lr = 0.001\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
        "\n",
        "    # Create data loaders\n",
        "    _bs = 32\n",
        "    train_loader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=_bs, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(list(zip(X_test, y_test)), batch_size=_bs)\n",
        "    print(f\"Batch size: {_bs}\", f\"lr: {_lr}\")\n",
        "    # Training loop\n",
        "    # num_epochs = 50\n",
        "    # for epoch in range(num_epochs):\n",
        "    #     print(\"Epoch \" + str(epoch))\n",
        "    #     train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "    #     test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "    #     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "    # Training loop\n",
        "    #num_epochs = 50\n",
        "    #for epoch in range(num_epochs):\n",
        "     #   train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "      #  test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "       # print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "    test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OcVTUn6entRm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9N8B8WXkUdJY"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), 'ResNet18_melspec_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qdOvcYZea_DZ"
      },
      "outputs": [],
      "source": [
        "# ! ls -lh /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VIsatL11bPU_"
      },
      "outputs": [],
      "source": [
        "# !cp '/content/ResNet18_melspec_50_32_0.001' '/content/drive/MyDrive/csci535/models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HMj9aENdbqeU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}