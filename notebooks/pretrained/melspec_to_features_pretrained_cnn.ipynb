{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm-A1pWMR3oa",
        "outputId": "294fec8e-f42e-4c9d-ac9b-406eb6df02fa"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SP57JsruRqX5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torchvision import models\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # ... (rest of your code)\n",
        "\n",
        "#     # Initialize the model\n",
        "#     model = models.vgg16(pretrained=True)  # Use pretrained VGG16\n",
        "#     model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  # Modify the first layer to take grayscale input\n",
        "#     num_classes = 3  # 3 classes for HAPPY, SAD, ANGRY\n",
        "#     num_features = model.classifier[6].in_features\n",
        "#     model.classifier[6] = nn.Linear(num_features, num_classes)  # Modify the final layer to match the number of classes\n",
        "\n",
        "#     # ... (rest of your code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = models.vgg16(pretrained=True)\n",
        "        # Revert the first layer to accept 3-channel input\n",
        "        original_first_layer = self.features.features[0]\n",
        "        self.features.features[0] = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        \n",
        "        # Copy weights from the original first layer to the new one, replicating the single channel weights across 3 channels\n",
        "        with torch.no_grad():\n",
        "            self.features.features[0].weight[:, :] = original_first_layer.weight[:, 0:1, :, :].repeat(1, 3, 1, 1) / 3\n",
        "        \n",
        "        # Modify the classifier to output the desired number of classes\n",
        "        num_features_before_fc = self.features.classifier[6].in_features\n",
        "        self.features.classifier[6] = nn.Linear(num_features_before_fc, num_classes)\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YYjikmeiRsNP"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match VGG input size\n",
        "        transforms.ToTensor(),  # Convert to tensor\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Duplicate grayscale image across three channels\n",
        "    ])\n",
        "    img_tensor = transform(img)\n",
        "    return img_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Y5pe1EXsSbpj"
      },
      "outputs": [],
      "source": [
        "def load_dataset(input_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    # List all files in the input folder\n",
        "    files = os.listdir(input_folder)\n",
        "    # Iterate over files in the folder\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            img_tensor = preprocess_image(input_path)\n",
        "            X.append(img_tensor)\n",
        "            # Extract label from filename (assuming filename is in format \"abc_IEO_label_xyz.png\")\n",
        "            label = filename.split(\"_\")[2]\n",
        "            if label == \"HAP\":\n",
        "                y.append(0)\n",
        "            elif label == \"SAD\":\n",
        "                y.append(1)\n",
        "            elif label == \"ANG\":\n",
        "                y.append(2)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Q5--S5JwTTw1"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ajpo8J6kTUjD"
      },
      "outputs": [],
      "source": [
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "92KI-5r7RtVi"
      },
      "outputs": [],
      "source": [
        "# def extract_features_from_folder(input_folder):\n",
        "#     # Initialize the model\n",
        "#     model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.to(device)\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "#     # List all files in the input folder\n",
        "#     files = os.listdir(input_folder)\n",
        "\n",
        "#     # Iterate over files in the folder\n",
        "#     for filename in files:\n",
        "#         if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n",
        "#             input_path = os.path.join(input_folder, filename)\n",
        "#             img_tensor = preprocess_image(input_path)\n",
        "#             img_tensor = img_tensor.to(device)\n",
        "#             with torch.no_grad():\n",
        "#                 output_features = model(img_tensor)\n",
        "#             print(f\"Features extracted for {filename}: {output_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eBB1OzDcRxDy"
      },
      "outputs": [],
      "source": [
        "# extract_features_from_folder('/content/drive/MyDrive/csci535/melspec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mnnN_idgTBu5"
      },
      "outputs": [],
      "source": [
        "# !python3 melspec_to_features_cnn.py /content/drive/MyDrive/csci535/melspec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CtiJ8-bS7vS",
        "outputId": "df7eebef-d619-4729-c32d-f49f6f5858b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of samples: 273\n",
            "Number of train samples: 191 Number of test samples: 82\n",
            "Batch size: 32 lr: 0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:05<00:00,  1.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.1027, Test Accuracy: 0.3537\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if input arguments are provided\n",
        "    # if len(sys.argv) != 2:\n",
        "    #     print(\"Usage: python melspec_to_features_cnn.py input_folder\")\n",
        "    #     sys.exit(1)\n",
        "\n",
        "    # input_folder = sys.argv[1]\n",
        "    input_folder = '/Users/aashigoyal/Desktop/vscode/aashimain/csci535/melspec'\n",
        "    # Check if input folder exists\n",
        "    if not os.path.exists(input_folder):\n",
        "        print(\"Input folder does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load dataset and split into train and test sets\n",
        "    X, y = load_dataset(input_folder)\n",
        "    print(f\"Total number of samples: {len(X)}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    print(f\"Number of train samples: {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
        "    # Initialize the model\n",
        "    model = CNN(num_classes=3)  # 3 classes for HAPPY, SAD, ANGRY\n",
        "    # Load the saved state dictionary\n",
        "    # state_dict = torch.load('/content/drive/MyDrive/csci535/models/ResNet18_melspec_50_32_0.001')\n",
        "    # state_dict = torch.load('/content/drive/MyDrive/csci535/models/ResNet18_video_50_32_0.001')\n",
        "    # Load the state dictionary into the model\n",
        "    # model.load_state_dict(state_dict)\n",
        "\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # model.to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    _lr = 0.001\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
        "\n",
        "    # Create data loaders\n",
        "    _bs = 32\n",
        "    # train_loader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=_bs, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(list(zip(X_test, y_test)), batch_size=_bs)\n",
        "    print(f\"Batch size: {_bs}\", f\"lr: {_lr}\")\n",
        "    # Training loop\n",
        "    # num_epochs = 50\n",
        "    # for epoch in range(num_epochs):\n",
        "    #     print(\"Epoch \" + str(epoch))\n",
        "    #     train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "    #     test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "    #     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "    test_loss, test_accuracy = test_model(model, criterion, test_loader, 'cpu')\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9N8B8WXkUdJY"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), 'ResNet18_melspec_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qdOvcYZea_DZ"
      },
      "outputs": [],
      "source": [
        "# ! ls -lh /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VIsatL11bPU_"
      },
      "outputs": [],
      "source": [
        "# !cp '/content/ResNet18_melspec_50_32_0.001' '/content/drive/MyDrive/csci535/models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMj9aENdbqeU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
