{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zihPBrfU_QKUBCMcoLztRadHDGSqQ-8-","timestamp":1712707246327}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IY4PsvJq4oPl","outputId":"0c05cb28-dddc-465a-8a97-ed6b2de4d1bb","executionInfo":{"status":"ok","timestamp":1712707473000,"user_tz":420,"elapsed":22065,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!python3 -m pip install einops\n","!python3 -m pip install facenet-pytorch\n","!python3 -m pip install face_alignment\n","!python3 -m pip install self_attention_cv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fos4gtTU4qM5","outputId":"71ef41e1-3e7e-40e2-ac5d-c3da8c878bf6","executionInfo":{"status":"ok","timestamp":1712707562723,"user_tz":420,"elapsed":89726,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n","Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.17.1+cu121)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (9.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2024.2.2)\n","Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet-pytorch) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet-pytorch) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet-pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet-pytorch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet-pytorch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet-pytorch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet-pytorch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision->facenet-pytorch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision->facenet-pytorch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision->facenet-pytorch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Collecting face_alignment\n","  Downloading face_alignment-1.4.1-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from face_alignment) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.25.2)\n","Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.11.4)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.19.3)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.8.0.76)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.66.2)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.58.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->face_alignment) (0.41.1)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (3.2.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2024.2.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (1.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (24.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->face_alignment) (12.4.127)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->face_alignment) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->face_alignment) (1.3.0)\n","Installing collected packages: face_alignment\n","Successfully installed face_alignment-1.4.1\n","Collecting self_attention_cv\n","  Downloading self_attention_cv-1.2.3-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (2.2.1+cu121)\n","Requirement already satisfied: torchvision>=0.8 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (0.17.1+cu121)\n","Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (0.7.0)\n","Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (1.25.2)\n","Requirement already satisfied: pytest>=6.2 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (7.4.4)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (2.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (24.0)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (1.4.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (1.2.0)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (2.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->self_attention_cv) (12.4.127)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8->self_attention_cv) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->self_attention_cv) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->self_attention_cv) (1.3.0)\n","Installing collected packages: self_attention_cv\n","Successfully installed self_attention_cv-1.2.3\n"]}]},{"cell_type":"code","source":["import cv2\n","import gc\n","import os\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from PIL import Image\n","from einops.layers.torch import Rearrange\n","from einops import rearrange\n","from facenet_pytorch import MTCNN\n","from self_attention_cv import TransformerEncoder\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from tqdm import tqdm\n","import face_alignment\n","import dlib\n","import requests"],"metadata":{"id":"0UY1RHcZ4YNw","executionInfo":{"status":"ok","timestamp":1712707576253,"user_tz":420,"elapsed":13534,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def extract_frame(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    mid_frame_index = frame_count // 2  # Index of the frame in the middle of the video\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_index)\n","    ret, frame = cap.read()\n","    if ret:\n","        cap.release()\n","        return frame\n","    else:\n","        cap.release()\n","        return None"],"metadata":{"id":"NiWEWAOF6b-S","executionInfo":{"status":"ok","timestamp":1712707576253,"user_tz":420,"elapsed":3,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def detect_face(frame):\n","    mtcnn = MTCNN()\n","    boxes, _ = mtcnn.detect(frame)\n","    if boxes is not None:\n","        # Assuming only one face in the frame\n","        box = boxes[0]\n","        x1, y1, x2, y2 = box\n","        # Crop the frame to the detected face\n","        cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n","        return cropped_frame\n","    else:\n","        return None"],"metadata":{"id":"l37ccKkO6cvt","executionInfo":{"status":"ok","timestamp":1712707576253,"user_tz":420,"elapsed":2,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Function to download the pretrained face alignment model if it doesn't exist\n","def download_face_alignment_model(url, save_path):\n","    if not os.path.exists(save_path):\n","        print(\"Downloading pretrained face alignment model...\")\n","        response = requests.get(url)\n","        with open(save_path, 'wb') as f:\n","            f.write(response.content)\n","        print(\"Download complete.\")\n","\n","# Specify the URL of the pretrained face alignment model\n","face_alignment_model_url = \"https://github.com/1adrianb/face-alignment-models/releases/download/2.0.1/2DFAN4-11f355bf06.pth.tar\"\n","\n","# Download the pretrained face alignment model if it doesn't exist\n","face_alignment_model_path = os.path.abspath(\"2DFAN4-11f355bf06.pth.tar\")\n","download_face_alignment_model(face_alignment_model_url, face_alignment_model_path)\n","\n","# Initialize face alignment model\n","fa = face_alignment.FaceAlignment(2, flip_input=False)  # 2 corresponds to 2D landmarks\n","\n","def align_face(frame):\n","    # Perform face alignment\n","    aligned_faces = fa.get_landmarks(frame)\n","    if aligned_faces is not None:\n","        aligned_face = aligned_faces[0]  # Assuming only one face in the frame\n","        return aligned_face\n","    else:\n","        return None\n"],"metadata":{"id":"GUr4HOzW6fmC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da6c5dfc-6a91-4a56-8cc4-f4fbb34da7c2","executionInfo":{"status":"ok","timestamp":1712707588650,"user_tz":420,"elapsed":12399,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading pretrained face alignment model...\n","Download complete.\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n","100%|██████████| 85.7M/85.7M [00:03<00:00, 22.9MB/s]\n","Downloading: \"https://www.adrianbulat.com/downloads/python-fan/3DFAN4-4a694010b9.zip\" to /root/.cache/torch/hub/checkpoints/3DFAN4-4a694010b9.zip\n","100%|██████████| 91.9M/91.9M [00:04<00:00, 23.6MB/s]\n"]}]},{"cell_type":"code","source":["# Define the preprocessing functions for video frames and spectrograms\n","def preprocess_image(frame):\n","    frame_pil = Image.fromarray(frame.astype('uint8'))\n","    frame_pil = frame_pil.convert('L')  # Convert to grayscale\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485], std=[0.229]),  # Normalize using ImageNet mean and std\n","    ])\n","    frame_tensor = transform(frame_pil)\n","    return frame_tensor"],"metadata":{"id":"wYOMbbU975Fm","executionInfo":{"status":"ok","timestamp":1712707588880,"user_tz":420,"elapsed":244,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def preprocess_spectrogram(image_path):\n","    img = Image.open(image_path).convert('L')  # Convert to grayscale\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),  # Resize to match ViT input size\n","        transforms.ToTensor(),           # Convert to tensor\n","    ])\n","    img_tensor = transform(img)\n","    return img_tensor"],"metadata":{"id":"BkbbyV3a-Bax","executionInfo":{"status":"ok","timestamp":1712707588880,"user_tz":420,"elapsed":3,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def load_spectrogram_dataset(input_folder):\n","    X = []\n","    y = []\n","    # List all files in the input folder\n","    files = os.listdir(input_folder)\n","    # Iterate over files in the folder\n","    for filename in files:\n","        if filename.endswith(\".png\"):  # Assuming mel spectrograms are stored as PNG files\n","            input_path = os.path.join(input_folder, filename)\n","            img_tensor = preprocess_spectrogram(input_path)\n","            X.append(img_tensor)\n","            # Extract label from filename (assuming filename is in format \"abc_IEO_label_xyz.png\")\n","            label = filename.split(\"_\")[2]\n","            if label == \"HAP\":\n","                y.append(0)\n","            elif label == \"SAD\":\n","                y.append(1)\n","            elif label == \"ANG\":\n","                y.append(2)\n","    return X, y"],"metadata":{"id":"TIqOKFzr6j0V","executionInfo":{"status":"ok","timestamp":1712707588880,"user_tz":420,"elapsed":3,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def load_dataset(input_folder):\n","    X = []\n","    y = []\n","    video_files = [file for file in os.listdir(input_folder) if file.endswith(\".flv\")]\n","    for video_file in tqdm(video_files):\n","        video_path = os.path.join(input_folder, video_file)\n","        frame = extract_frame(video_path)\n","        if frame is not None:\n","            cropped_face = detect_face(frame)\n","            if cropped_face is not None:\n","                preprocessed_face = preprocess_image(cropped_face)\n","                X.append(preprocessed_face)\n","                label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n","                if label == \"HAP\":\n","                    y.append(0)\n","                elif label == \"SAD\":\n","                    y.append(1)\n","                elif label == \"ANG\":\n","                    y.append(2)\n","            else:\n","                print(f\"No face detected in {video_file}. Skipping.\")\n","        else:\n","            print(f\"Failed to extract frame from {video_file}. Skipping.\")\n","    return X, y"],"metadata":{"id":"n4Xum7nf6lfg","executionInfo":{"status":"ok","timestamp":1712707588880,"user_tz":420,"elapsed":2,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Define the ConcatDataset class to concatenate video frame and spectrogram tensors\n","class ConcatDataset(torch.utils.data.Dataset):\n","    def __init__(self, X1, X2, y, modality='multimodal'):\n","        self.X1 = X1\n","        self.X2 = X2\n","        self.y = y\n","        self.modality = modality\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        img1 = self.X1[idx]\n","        img2 = self.X2[idx]\n","        label = self.y[idx]\n","        concatenated_img = torch.cat((img1, img2), dim=0)  # Concatenate along 0 dimension\n","        if self.modality == 'visual':\n","          return img1, label\n","        if self.modality == 'audio':\n","          return img2, label\n","        return concatenated_img, label # concatenate modalities"],"metadata":{"id":"q8gd6HoS--Rj","executionInfo":{"status":"ok","timestamp":1712707588880,"user_tz":420,"elapsed":2,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train_model(model, criterion, optimizer, train_loader, device):\n","    model.train()\n","    running_loss = 0.0\n","    correct_preds = 0\n","    total_preds = 0\n","    for inputs, labels in tqdm(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        correct_preds += (predicted == labels).sum().item()\n","        total_preds += labels.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    accuracy = correct_preds / total_preds\n","    return epoch_loss, accuracy"],"metadata":{"id":"3TBMdqtI6oZ0","executionInfo":{"status":"ok","timestamp":1712707588880,"user_tz":420,"elapsed":2,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def test_model(model, criterion, test_loader, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_preds = 0\n","    total_preds = 0\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(test_loader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            correct_preds += (predicted == labels).sum().item()\n","            total_preds += labels.size(0)\n","    epoch_loss = running_loss / len(test_loader.dataset)\n","    accuracy = correct_preds / total_preds\n","    return epoch_loss, accuracy"],"metadata":{"id":"aq6skVDB6p6M","executionInfo":{"status":"ok","timestamp":1712707588880,"user_tz":420,"elapsed":2,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class ViT(nn.Module):\n","    # ViT architecture adapted from here - https://theaisummer.com/vision-transformer/\n","    def __init__(self, *,\n","                 img_dim,\n","                 in_channels=3,\n","                 patch_dim=16,\n","                 num_classes=3,\n","                 dim=512,\n","                 blocks=6,\n","                 heads=4,\n","                 dim_linear_block=1024,\n","                 dim_head=None,\n","                 dropout=0, transformer=None, classification=True):\n","        \"\"\"\n","        Args:\n","            img_dim: the spatial image size\n","            in_channels: number of img channels\n","            patch_dim: desired patch dim\n","            num_classes: classification task classes\n","            dim: the linear layer's dim to project the patches for MHSA\n","            blocks: number of transformer blocks\n","            heads: number of heads\n","            dim_linear_block: inner dim of the transformer linear block\n","            dim_head: dim head in case you want to define it. defaults to dim/heads\n","            dropout: for pos emb and transformer\n","            transformer: in case you want to provide another transformer implementation\n","            classification: creates an extra CLS token\n","        \"\"\"\n","        super().__init__()\n","        assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible'\n","        self.p = patch_dim\n","        self.classification = classification\n","        tokens = (img_dim // patch_dim) ** 2\n","        self.token_dim = in_channels * (patch_dim ** 2)\n","        self.dim = dim\n","        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n","        self.project_patches = nn.Linear(self.token_dim, dim)\n","\n","        self.emb_dropout = nn.Dropout(dropout)\n","        if self.classification:\n","            self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","            self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n","            self.mlp_head = nn.Linear(dim, num_classes)\n","        else:\n","            self.pos_emb1D = nn.Parameter(torch.randn(tokens, dim))\n","\n","        if transformer is None:\n","            self.transformer = TransformerEncoder(dim, blocks=blocks, heads=heads,\n","                                                  dim_head=self.dim_head,\n","                                                  dim_linear_block=dim_linear_block,\n","                                                  dropout=dropout)\n","        else:\n","            self.transformer = transformer\n","\n","    def expand_cls_to_batch(self, batch):\n","        \"\"\"\n","        Args:\n","            batch: batch size\n","        Returns: cls token expanded to the batch size\n","        \"\"\"\n","        return self.cls_token.expand([batch, -1, -1])\n","\n","    def forward(self, img, mask=None):\n","        batch_size = img.shape[0]\n","        img_patches = rearrange(\n","            img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n","                                patch_x=self.p, patch_y=self.p)\n","        # project patches with linear layer + add pos emb\n","        img_patches = self.project_patches(img_patches)\n","\n","        if self.classification:\n","            img_patches = torch.cat(\n","                (self.expand_cls_to_batch(batch_size), img_patches), dim=1)\n","\n","        patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n","\n","        # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n","        y = self.transformer(patch_embeddings, mask)\n","\n","        if self.classification:\n","            # we index only the cls token for classification.\n","            return self.mlp_head(y[:, 0, :])\n","        else:\n","            return y"],"metadata":{"id":"mhjpuUMgo3KU","executionInfo":{"status":"ok","timestamp":1712707588880,"user_tz":420,"elapsed":2,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAHGApwK4N92","outputId":"842bb376-1103-4c11-e66f-1d0f9e02193b","executionInfo":{"status":"ok","timestamp":1712707638361,"user_tz":420,"elapsed":49483,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 273/273 [00:40<00:00,  6.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total number of samples: 273\n","Number of train samples (video): 191 Number of test samples: 82\n","Number of train samples (audio): 191 Number of test samples: 82\n"]}],"source":["# Define input_folder and input_folder_spec\n","input_folder = '/content/drive/MyDrive/videos'\n","input_folder_spec = '/content/drive/MyDrive/melspec'\n","\n","# Check if input folder exists\n","if not os.path.exists(input_folder):\n","    print(\"Input folder does not exist.\")\n","    sys.exit(1)\n","# Check if input folder exists\n","if not os.path.exists(input_folder_spec):\n","    print(\"Input folder does not exist.\")\n","    sys.exit(1)\n","\n","# Load dataset and split into train and test sets\n","X, y = load_dataset(input_folder)\n","X_spec, y_spec = load_spectrogram_dataset(input_folder_spec)\n","\n","# Split the data into train and test sets\n","print(f\"Total number of samples: {len(X)}\")\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","print(f\"Number of train samples (video): {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n","X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(X_spec, y_spec, test_size=0.3, random_state=42)\n","print(f\"Number of train samples (audio): {len(X_train_spec)}\", f\"Number of test samples: {len(X_test_spec)}\")"]},{"cell_type":"code","source":["def train_ViT(_modality):\n","  # Adjust input channels as per modality\n","  if _modality == 'multimodal':\n","    _input_channels = 2\n","  else:\n","    _input_channels = 1\n","\n","  # Initialize the ViT model\n","  model = ViT(img_dim=224,  # Image dimension\n","              in_channels=_input_channels,  # Number of input channels\n","              patch_dim=16,  # Patch dimension\n","              num_classes=3,  #  # 3 classes for HAPPY, SAD, ANGRY\n","              dim=768,  # Dimensionality of the token embeddings\n","              blocks=6,  # Number of transformer blocks\n","              heads=4,  # Number of attention heads\n","              dim_linear_block=1024,  # Dimensionality of the linear block\n","              dropout=0.4,  # Dropout rate\n","              classification=True)  # Whether or not to include a classification token\n","\n","  # Define device\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model.to(device)\n","\n","  # Define loss function and optimizer\n","  _lr = 0.01\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=_lr)\n","\n","  # Concatenate datasets if multimodal\n","  train_dataset = ConcatDataset(X_train, X_train_spec, y_train, modality = _modality)\n","  test_dataset = ConcatDataset(X_test, X_test_spec, y_test, modality = _modality)\n","\n","  # Create data loaders\n","  _bs = 32\n","\n","  # Create data loaders\n","  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=_bs, shuffle=True)\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_bs)\n","\n","  print(f\"\\n\\nBatch size: {_bs}\", f\"lr: {_lr}\")\n","\n","  # Training loop\n","  num_epochs = 50\n","  print(f\"Training ViT for \\\"{_modality}\\\" pipeline ...\\n------------------------------------------------\\n\")\n","  for epoch in range(num_epochs):\n","      print(\"Epoch \" + str(epoch))\n","      train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n","      test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n","      print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","\n","  # Save the model\n","  if _modality == 'multimodal':\n","    torch.save(model.state_dict(), 'ViT_audio_video_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))\n","  elif _modality == 'audio':\n","    torch.save(model.state_dict(), 'ViT_audio_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))\n","  elif _modality == 'visual':\n","    torch.save(model.state_dict(), 'ViT_video_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))\n","  else:\n","    print(\"Improper modality provided!\")\n","\n","  return train_loss, train_accuracy, test_loss, test_accuracy"],"metadata":{"id":"wzuHjENY9Dpa","executionInfo":{"status":"ok","timestamp":1712707638361,"user_tz":420,"elapsed":16,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Define modalities\n","# _modality = ['visual', 'audio', 'multimodal']"],"metadata":{"id":"CXy5mkXruLRr","executionInfo":{"status":"ok","timestamp":1712707638361,"user_tz":420,"elapsed":15,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Train ViT\n","# scores = {}\n","# for _m in _modality:\n","#   train_loss, train_accuracy, test_loss, test_accuracy = train_ViT(_m)\n","#   scores[_m] = [train_loss, train_accuracy, test_loss, test_accuracy]"],"metadata":{"id":"eaq2Nfvs9IZm","executionInfo":{"status":"ok","timestamp":1712707638361,"user_tz":420,"elapsed":15,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Print results\n","# print(\"\\nResults\\n---------------\\n\")\n","# for _m, val in scores.items():\n","#   print(f\"Modality: {_m}, Train Loss: {val[0]:.4f}, Train Accuracy: {val[1]:.4f}, Test Loss: {val[2]:.4f}, Test Accuracy: {val[3]:.4f}\")"],"metadata":{"id":"XNOX2sOP0E3j","executionInfo":{"status":"ok","timestamp":1712707638361,"user_tz":420,"elapsed":15,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Copy trained models to GDrive\n","# !cp 'ViT_audio_video_50_16_0.0001' '/content/drive/MyDrive/csci535/models'\n","# !cp 'ViT_audio_50_16_0.0001' '/content/drive/MyDrive/csci535/models'\n","# !cp 'ViT_video_50_16_0.0001' '/content/drive/MyDrive/csci535/models'"],"metadata":{"id":"7eQaDTdZ9Qak","executionInfo":{"status":"ok","timestamp":1712707638361,"user_tz":420,"elapsed":15,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def cross_test_ViT(_modality_model, _modality_features):\n","  _input_channels = 1\n","  model = ViT(img_dim=224,  # Image dimension\n","                in_channels=_input_channels,  # Number of input channels\n","                patch_dim=16,  # Patch dimension\n","                num_classes=3,  #  # 3 classes for HAPPY, SAD, ANGRY\n","                dim=768,  # Dimensionality of the token embeddings\n","                blocks=6,  # Number of transformer blocks\n","                heads=4,  # Number of attention heads\n","                dim_linear_block=1024,  # Dimensionality of the linear block\n","                dropout=0.4,  # Dropout rate\n","                classification=True)  # Whether or not to include a classification token\n","\n","  state_dict = torch.load('/content/drive/MyDrive/csci535_aashi/models/ViT_' + _modality_model + '_50_8_0.0001')\n","  model.load_state_dict(state_dict)\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model.to(device)\n","  # Define loss function and optimizer\n","  _lr = 0.0001\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=_lr)\n","\n","  test_dataset = ConcatDataset(X_test, X_test_spec, y_test, modality = _modality_features)\n","\n","  # Create data loaders\n","  _bs = 8\n","\n","  # Create data loaders\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_bs)\n","  print(f\"\\nTesting {_modality_model}-trained ViT pipeline on {_modality_features} features ...\\n---------------------------------------------------------\\n\")\n","  test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n","  print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"],"metadata":{"id":"XzFnQhXhmCiI","executionInfo":{"status":"ok","timestamp":1712707638361,"user_tz":420,"elapsed":15,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Peform cross-test\n","cross_test_ViT('audio', 'visual')\n","cross_test_ViT('video', 'audio')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIkch1Ew7aXb","outputId":"ac4d6f88-3f8d-4931-92b4-acafdc8a51e9","executionInfo":{"status":"ok","timestamp":1712707647561,"user_tz":420,"elapsed":9215,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing audio-trained ViT pipeline on visual features ...\n","---------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:00<00:00, 13.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 2.0405, Test Accuracy: 0.3415\n","\n","Testing video-trained ViT pipeline on audio features ...\n","---------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:00<00:00, 32.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 3.2201, Test Accuracy: 0.3537\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Perform straight-test for sanity check\n","cross_test_ViT('audio', 'audio')\n","cross_test_ViT('video', 'visual')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ciPd32Pe9EPB","outputId":"553c5a0f-a543-4ba5-a9fe-91b272f4df05","executionInfo":{"status":"ok","timestamp":1712707648849,"user_tz":420,"elapsed":1301,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing audio-trained ViT pipeline on audio features ...\n","---------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:00<00:00, 44.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 1.2184, Test Accuracy: 0.4024\n","\n","Testing video-trained ViT pipeline on visual features ...\n","---------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:00<00:00, 43.99it/s]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 2.3896, Test Accuracy: 0.6098\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Clear memory\n","gc.collect()"],"metadata":{"id":"Q5SeIAOnTI-E","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c1223a88-6176-40c1-fe9e-f5c669bfd4c6","executionInfo":{"status":"ok","timestamp":1712707649064,"user_tz":420,"elapsed":217,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[],"metadata":{"id":"luOIsFr88I_b","executionInfo":{"status":"ok","timestamp":1712707649064,"user_tz":420,"elapsed":5,"user":{"displayName":"Aashi Goyal","userId":"02192058950231989493"}}},"execution_count":24,"outputs":[]}]}