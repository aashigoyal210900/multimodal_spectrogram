{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount GDrive"
      ],
      "metadata": {
        "id": "b7puxPzEh6Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiorwfJxh5yf",
        "outputId": "790b1d1a-0eb4-4e2f-af09-d748d82369bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unzip Data from GDrive"
      ],
      "metadata": {
        "id": "Q6YUcijajDEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q '/content/drive/MyDrive/csci535/Videos7439.zip'\n",
        "!unzip -q '/content/drive/MyDrive/csci535/Spectrograms7439.zip'"
      ],
      "metadata": {
        "id": "Yn4fkI7Ci355"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNn5eCFcCPyu"
      },
      "source": [
        "#### Check the number of video files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_SEe2eQ2l5H",
        "outputId": "2c0bcfcb-3960-4c7d-9b77-9bc62400f464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7439\n"
          ]
        }
      ],
      "source": [
        "!ls 'Videos7439' | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VijEs9d_CSdf"
      },
      "source": [
        "#### Check the number of spectrogram files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3tDAXzj2vKr",
        "outputId": "005ac39a-b5cc-4ec9-cfcb-09f40855003d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7439\n"
          ]
        }
      ],
      "source": [
        "!ls 'Spectrograms7439' | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieNy3uK4CVUA"
      },
      "source": [
        "#### Assign video and spectrogram directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "adMqPLKu2koa"
      },
      "outputs": [],
      "source": [
        "videos_folder = 'Videos7439'\n",
        "spectrograms_folder = 'Spectrograms7439'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OXsJTX1CfG-"
      },
      "source": [
        "#### Install some CV libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fos4gtTU4qM5",
        "outputId": "c822c91a-271a-4936-b02b-0434f89605ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n",
            "Collecting Pillow<10.3.0,>=10.2.0 (from facenet-pytorch)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.17.1+cu121)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Installing collected packages: Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, facenet-pytorch\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting face_alignment\n",
            "  Downloading face_alignment-1.4.1-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from face_alignment) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.10/dist-packages (from face_alignment) (1.11.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.8.0.76)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from face_alignment) (4.66.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from face_alignment) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->face_alignment) (0.41.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (3.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (10.2.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (2024.4.24)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->face_alignment)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->face_alignment)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->face_alignment)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->face_alignment)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->face_alignment)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->face_alignment)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (10.3.2.106)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->face_alignment)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->face_alignment)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->face_alignment) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->face_alignment) (12.4.127)\n",
            "Collecting pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 (from scikit-image->face_alignment)\n",
            "  Downloading Pillow-10.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->face_alignment) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->face_alignment) (1.3.0)\n",
            "Installing collected packages: pillow, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, face_alignment\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 10.2.0\n",
            "    Uninstalling pillow-10.2.0:\n",
            "      Successfully uninstalled pillow-10.2.0\n",
            "Successfully installed face_alignment-1.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 pillow-10.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "91a5cdd0867f4a88b7539dbb86a4f5d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting self_attention_cv\n",
            "  Downloading self_attention_cv-1.2.3-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (0.17.1+cu121)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (1.25.2)\n",
            "Requirement already satisfied: pytest>=6.2 in /usr/local/lib/python3.10/dist-packages (from self_attention_cv) (7.4.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2->self_attention_cv) (2.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->self_attention_cv) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->self_attention_cv) (12.4.127)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8->self_attention_cv) (10.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->self_attention_cv) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->self_attention_cv) (1.3.0)\n",
            "Installing collected packages: self_attention_cv\n",
            "Successfully installed self_attention_cv-1.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install facenet-pytorch\n",
        "!pip install face_alignment\n",
        "!pip install self_attention_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD1hVNvmClgX"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0UY1RHcZ4YNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "ab2de6cc-466f-4371-a98b-e152ee0f2818"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'facenet_pytorch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dd7374fc128b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRearrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself_attention_cv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'facenet_pytorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from einops.layers.torch import Rearrange\n",
        "from einops import rearrange\n",
        "from facenet_pytorch import MTCNN\n",
        "from self_attention_cv import TransformerEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import face_alignment\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGcSBK2ZNeiK"
      },
      "source": [
        "#### Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiWEWAOF6b-S"
      },
      "outputs": [],
      "source": [
        "def extract_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    mid_frame_index = frame_count // 2  # Index of the frame in the middle of the video\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_index)\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        cap.release()\n",
        "        return frame\n",
        "    else:\n",
        "        cap.release()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l37ccKkO6cvt"
      },
      "outputs": [],
      "source": [
        "def detect_face(frame):\n",
        "    mtcnn = MTCNN()\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    if boxes is not None:\n",
        "        # Assuming only one face in the frame\n",
        "        box = boxes[0]\n",
        "        x1, y1, x2, y2 = box\n",
        "        # Crop the frame to the detected face\n",
        "        cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "        return cropped_frame\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYOMbbU975Fm"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing functions for video frames and spectrograms\n",
        "def preprocess_image(frame):\n",
        "    frame_pil = Image.fromarray(frame.astype('uint8'))\n",
        "    frame_pil = frame_pil.convert('L')  # Convert to grayscale\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229]),  # Normalize using ImageNet mean and std\n",
        "    ])\n",
        "    frame_tensor = transform(frame_pil)\n",
        "    return frame_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkbbyV3a-Bax"
      },
      "outputs": [],
      "source": [
        "def preprocess_spectrogram(image_path):\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match ViT input size\n",
        "        transforms.ToTensor(),           # Convert to tensor\n",
        "    ])\n",
        "    img_tensor = transform(img)\n",
        "    return img_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Xum7nf6lfg"
      },
      "outputs": [],
      "source": [
        "def load_dataset(videos_folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    skipped_files = []\n",
        "    video_files = [file for file in sorted(os.listdir(videos_folder)) if file.endswith(\".flv\")]\n",
        "    for video_file in tqdm(video_files):\n",
        "        video_path = os.path.join(videos_folder, video_file)\n",
        "        frame = extract_frame(video_path)\n",
        "        if frame is not None:\n",
        "            cropped_face = detect_face(frame)\n",
        "            if cropped_face is not None:\n",
        "                preprocessed_face = preprocess_image(cropped_face)\n",
        "                X.append(preprocessed_face)\n",
        "                label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
        "                if label == \"HAP\":\n",
        "                    y.append(0)\n",
        "                elif label == \"SAD\":\n",
        "                    y.append(1)\n",
        "                elif label == \"ANG\":\n",
        "                    y.append(2)\n",
        "                elif label == \"DIS\":\n",
        "                    y.append(3)\n",
        "                elif label == \"FEA\":\n",
        "                    y.append(4)\n",
        "                elif label == \"NEU\":\n",
        "                    y.append(5)\n",
        "            else:\n",
        "                print(f\"No face detected in {video_file}. Skipping.\")\n",
        "                skipped_files.append(video_file[:-3])\n",
        "        else:\n",
        "            print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
        "            skipped_files.append(video_file[:-3])\n",
        "    return X, y, skipped_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIqOKFzr6j0V"
      },
      "outputs": [],
      "source": [
        "def load_spectrogram_dataset(spectrograms_folder, skipped_files):\n",
        "    X = []\n",
        "    y = []\n",
        "    # List all files in the input folder\n",
        "    files = sorted(os.listdir(spectrograms_folder))\n",
        "    # Iterate over files in the folder\n",
        "    for filename in tqdm(files):\n",
        "        if filename.endswith(\".png\") and filename[:-3] not in skipped_files:  # Assuming mel spectrograms are stored as PNG files\n",
        "            input_path = os.path.join(spectrograms_folder, filename)\n",
        "            img_tensor = preprocess_spectrogram(input_path)\n",
        "            X.append(img_tensor)\n",
        "            # Extract label from filename (assuming filename is in format \"abc_IEO_label_xyz.png\")\n",
        "            label = filename.split(\"_\")[2]\n",
        "            if label == \"HAP\":\n",
        "                y.append(0)\n",
        "            elif label == \"SAD\":\n",
        "                y.append(1)\n",
        "            elif label == \"ANG\":\n",
        "                y.append(2)\n",
        "            elif label == \"DIS\":\n",
        "                y.append(3)\n",
        "            elif label == \"FEA\":\n",
        "                y.append(4)\n",
        "            elif label == \"NEU\":\n",
        "                y.append(5)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8gd6HoS--Rj"
      },
      "outputs": [],
      "source": [
        "# Define the ConcatDataset class to concatenate video frame and spectrogram tensors\n",
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X1, X2, y, modality='multimodal', presaved=False):\n",
        "        self.X1 = X1\n",
        "        self.X2 = X2\n",
        "        self.y = y\n",
        "        self.modality = modality\n",
        "        self.presaved = presaved\n",
        "    def __len__(self):\n",
        "        if self.modality == 'audio':\n",
        "          return len(self.X2)\n",
        "        if self.modality == 'visual':\n",
        "          return len(self.X1)\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if not self.presaved:\n",
        "          img1 = self.X1[idx]\n",
        "          img2 = self.X2[idx]\n",
        "          label = self.y[idx]\n",
        "        else:\n",
        "          img1 = torch.from_numpy(self.X1[idx]).float()  # Convert numpy array to torch tensor\n",
        "          img2 = torch.from_numpy(self.X2[idx]).float()  # Convert numpy array to torch tensor\n",
        "          label = torch.tensor(self.y[idx])  # Convert numpy array to torch tensor\n",
        "\n",
        "        concatenated_img = torch.cat((img1, img2), dim=0)  # Concatenate along 0 dimension\n",
        "        if self.modality == 'visual':\n",
        "          return img1, label\n",
        "        if self.modality == 'audio':\n",
        "          return img2, label\n",
        "        return concatenated_img, label # concatenate modalities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGRELOq6Nkcj"
      },
      "source": [
        "#### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TBMdqtI6oZ0"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zflWu-4dNnAX"
      },
      "source": [
        "#### Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq6skVDB6p6M"
      },
      "outputs": [],
      "source": [
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNnr6W_ZNuOz"
      },
      "source": [
        "#### ViT Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhjpuUMgo3KU"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    # ViT architecture adapted from here - https://theaisummer.com/vision-transformer/\n",
        "    def __init__(self, *,\n",
        "                 img_dim,\n",
        "                 in_channels=3,\n",
        "                 patch_dim=16,\n",
        "                 num_classes=6, # full-scale CREMA-D\n",
        "                 dim=512,\n",
        "                 blocks=6,\n",
        "                 heads=4,\n",
        "                 dim_linear_block=1024,\n",
        "                 dim_head=None,\n",
        "                 dropout=0, transformer=None, classification=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dim: the spatial image size\n",
        "            in_channels: number of img channels\n",
        "            patch_dim: desired patch dim\n",
        "            num_classes: classification task classes\n",
        "            dim: the linear layer's dim to project the patches for MHSA\n",
        "            blocks: number of transformer blocks\n",
        "            heads: number of heads\n",
        "            dim_linear_block: inner dim of the transformer linear block\n",
        "            dim_head: dim head in case you want to define it. defaults to dim/heads\n",
        "            dropout: for pos emb and transformer\n",
        "            transformer: in case you want to provide another transformer implementation\n",
        "            classification: creates an extra CLS token\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible'\n",
        "        self.p = patch_dim\n",
        "        self.classification = classification\n",
        "        tokens = (img_dim // patch_dim) ** 2\n",
        "        self.token_dim = in_channels * (patch_dim ** 2)\n",
        "        self.dim = dim\n",
        "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
        "        self.project_patches = nn.Linear(self.token_dim, dim)\n",
        "\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "        if self.classification:\n",
        "            self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "            self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n",
        "            self.mlp_head = nn.Linear(dim, num_classes)\n",
        "        else:\n",
        "            self.pos_emb1D = nn.Parameter(torch.randn(tokens, dim))\n",
        "\n",
        "        if transformer is None:\n",
        "            self.transformer = TransformerEncoder(dim, blocks=blocks, heads=heads,\n",
        "                                                  dim_head=self.dim_head,\n",
        "                                                  dim_linear_block=dim_linear_block,\n",
        "                                                  dropout=dropout)\n",
        "        else:\n",
        "            self.transformer = transformer\n",
        "\n",
        "    def expand_cls_to_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch: batch size\n",
        "        Returns: cls token expanded to the batch size\n",
        "        \"\"\"\n",
        "        return self.cls_token.expand([batch, -1, -1])\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        batch_size = img.shape[0]\n",
        "        img_patches = rearrange(\n",
        "            img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
        "                                patch_x=self.p, patch_y=self.p)\n",
        "        # project patches with linear layer + add pos emb\n",
        "        img_patches = self.project_patches(img_patches)\n",
        "\n",
        "        if self.classification:\n",
        "            img_patches = torch.cat(\n",
        "                (self.expand_cls_to_batch(batch_size), img_patches), dim=1)\n",
        "\n",
        "        patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n",
        "\n",
        "        # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
        "        y = self.transformer(patch_embeddings, mask)\n",
        "\n",
        "        if self.classification:\n",
        "            # we index only the cls token for classification.\n",
        "            return self.mlp_head(y[:, 0, :])\n",
        "        else:\n",
        "            return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check that the videos and spectrograms are in one-to-one correspondence"
      ],
      "metadata": {
        "id": "tzBGjd6CmgTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert([file[:-3] for file in sorted(os.listdir(videos_folder))] == [file[:-3] for file in sorted(os.listdir(spectrograms_folder))])"
      ],
      "metadata": {
        "id": "3zGJMimyl1dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqhwv6e0OOZX"
      },
      "source": [
        "#### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAHGApwK4N92",
        "outputId": "0850c470-ba82-4b6b-a841-dc0bfff4cea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 2638/7439 [07:07<09:58,  8.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1033_ITH_HAP_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 2662/7439 [07:11<09:49,  8.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1033_MTI_HAP_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 2679/7439 [07:13<09:34,  8.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1033_TSI_FEA_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 2959/7439 [07:56<13:43,  5.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1037_IOM_FEA_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 3165/7439 [08:29<09:27,  7.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1039_TIE_FEA_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 6185/7439 [16:43<04:27,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1076_MTI_SAD_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 6456/7439 [17:30<03:19,  4.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1080_DFA_ANG_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 6470/7439 [17:33<03:40,  4.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1080_IEO_FEA_MD.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 6478/7439 [17:34<03:33,  4.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1080_IOM_ANG_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 6530/7439 [17:45<02:20,  6.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in 1080_TSI_HAP_XX.flv. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7439/7439 [20:18<00:00,  6.10it/s]\n",
            "100%|██████████| 7439/7439 [01:48<00:00, 68.40it/s]\n"
          ]
        }
      ],
      "source": [
        "_fullscale = True # Run fullscale experiment?\n",
        "_presaved = False # Use presaved data .npy files?\n",
        "\n",
        "# Check if data folders exists\n",
        "if not os.path.exists(videos_folder):\n",
        "    print(\"Videos folder does not exist.\")\n",
        "    sys.exit(1)\n",
        "if not os.path.exists(spectrograms_folder):\n",
        "    print(\"Spectrograms folder does not exist.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Load dataset and split into train and test sets\n",
        "if _presaved:\n",
        "  X = np.load('/content/drive/MyDrive/csci535/models/X_7439.npy', mmap_mode='r')\n",
        "  y = np.load('/content/drive/MyDrive/csci535/models/y_7439.npy', mmap_mode='r')\n",
        "  X_spec = np.load('/content/drive/MyDrive/csci535/models/X_spec_7439.npy', mmap_mode='r')\n",
        "  y_spec = np.load('/content/drive/MyDrive/csci535/models/y_spec_7439.npy', mmap_mode='r')\n",
        "else:\n",
        "  X, y, skipped_files = load_dataset(videos_folder)\n",
        "  X_spec, y_spec = load_spectrogram_dataset(spectrograms_folder, skipped_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save data as .npy files (if needed)"
      ],
      "metadata": {
        "id": "VaJ1vf4fvBze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "3gzXGfQPSBAC"
      },
      "outputs": [],
      "source": [
        "# Save X, y, X_spec, y_spec\n",
        "np.save('X_7439.npy', np.array(X))\n",
        "np.save('y_7439.npy', np.array(y))\n",
        "np.save('X_spec_7439.npy', np.array(X_spec))\n",
        "np.save('y_spec_7439.npy', np.array(y_spec))\n",
        "!cp 'X_7439.npy' '/content/drive/MyDrive/csci535/models/'\n",
        "!cp 'y_7439.npy' '/content/drive/MyDrive/csci535/models/'\n",
        "!cp 'X_spec_7439.npy' '/content/drive/MyDrive/csci535/models/'\n",
        "!cp 'y_spec_7439.npy' '/content/drive/MyDrive/csci535/models/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KSU8u_XOSxe"
      },
      "source": [
        "#### Split Data into Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXnZWva25hlf",
        "outputId": "418983cd-5172-44c0-8ec5-76717d05e668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 7429\n",
            "Number of train samples (video): 5200 Number of test samples: 2229\n",
            "Number of train samples (audio): 5200 Number of test samples: 2229\n"
          ]
        }
      ],
      "source": [
        "# Split the data into train and test sets\n",
        "print(f\"Total number of samples: {len(X)}\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(f\"Number of train samples (video): {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
        "X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(X_spec, y_spec, test_size=0.3, random_state=42)\n",
        "print(f\"Number of train samples (audio): {len(X_train_spec)}\", f\"Number of test samples: {len(X_test_spec)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUs38TnUDJIx"
      },
      "source": [
        "#### Check that the number of frames and spectrograms are equal and that the train + test samples add up to total samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "m8GNHaIICzPl"
      },
      "outputs": [],
      "source": [
        "assert(len(X_train) == len(X_train_spec) and len(X_test) == len(X_test_spec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn7J6MNNOqyA"
      },
      "source": [
        "#### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wzuHjENY9Dpa"
      },
      "outputs": [],
      "source": [
        "def train_ViT(_modality):\n",
        "  # Adjust input channels as per modality\n",
        "  if _modality == 'multimodal':\n",
        "    _input_channels = 2\n",
        "  else:\n",
        "    _input_channels = 1\n",
        "\n",
        "  # Initialize the ViT model\n",
        "  model = ViT(img_dim=224,  # Image dimension\n",
        "              in_channels=_input_channels,  # Number of input channels\n",
        "              patch_dim=16,  # Patch dimension\n",
        "              num_classes=6,  # 6 classes for HAPPY, SAD, ANGRY, DISGUST, FEAR, NEUTRAL\n",
        "              dim=768,  # Dimensionality of the token embeddings\n",
        "              blocks=6,  # Number of transformer blocks\n",
        "              heads=4,  # Number of attention heads\n",
        "              dim_linear_block=1024,  # Dimensionality of the linear block\n",
        "              dropout=0.4,  # Dropout rate\n",
        "              classification=True)  # Whether or not to include a classification token\n",
        "\n",
        "  # Define device\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  # Define loss function and optimizer\n",
        "  _lr = 0.0001\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
        "\n",
        "  # Concatenate datasets if multimodal\n",
        "  train_dataset = ConcatDataset(X_train, X_train_spec, y_train, modality = _modality, presaved = _presaved)\n",
        "  test_dataset = ConcatDataset(X_test, X_test_spec, y_test, modality = _modality, presaved = _presaved)\n",
        "\n",
        "  # Create data loaders\n",
        "  _bs = 16\n",
        "\n",
        "  # Create data loaders\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=_bs, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_bs)\n",
        "\n",
        "  print(f\"\\n\\nBatch size: {_bs}\", f\"lr: {_lr}\")\n",
        "\n",
        "  # Training loop\n",
        "  num_epochs = 50\n",
        "  print(f\"Training ViT for \\\"{_modality}\\\" pipeline ...\\n------------------------------------------------\\n\")\n",
        "  for epoch in range(num_epochs):\n",
        "      print(\"Epoch \" + str(epoch))\n",
        "      train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
        "      test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
        "      print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "  # Save the model\n",
        "  if _modality == 'multimodal':\n",
        "    torch.save(model.state_dict(), 'ViT_audio_video_fullscale_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))\n",
        "  elif _modality == 'audio':\n",
        "    torch.save(model.state_dict(), 'ViT_audio_fullscale_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))\n",
        "  elif _modality == 'visual':\n",
        "    torch.save(model.state_dict(), 'ViT_video_fullscale_'+str(num_epochs)+'_'+str(_bs)+'_'+str(_lr))\n",
        "  else:\n",
        "    print(\"Improper modality provided!\")\n",
        "\n",
        "  return train_loss, train_accuracy, test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaxDFf2pOtTa"
      },
      "source": [
        "#### Choose modalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "CXy5mkXruLRr"
      },
      "outputs": [],
      "source": [
        "# Define modalities\n",
        "_modality = ['visual', 'audio', 'multimodal']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBJ5KpnFOu6D"
      },
      "source": [
        "#### Train ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaq2Nfvs9IZm",
        "outputId": "65044544-72b8-4968-920f-10de441776cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Batch size: 16 lr: 0.0001\n",
            "Training ViT for \"audio\" pipeline ...\n",
            "------------------------------------------------\n",
            "\n",
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:49<00:00,  6.62it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 19.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 1.6541, Train Accuracy: 0.3023, Test Loss: 1.5680, Test Accuracy: 0.3540\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:50<00:00,  6.45it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50, Train Loss: 1.5593, Train Accuracy: 0.3504, Test Loss: 1.4906, Test Accuracy: 0.3952\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.16it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50, Train Loss: 1.5210, Train Accuracy: 0.3542, Test Loss: 1.5114, Test Accuracy: 0.3872\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.15it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50, Train Loss: 1.4958, Train Accuracy: 0.3737, Test Loss: 1.4390, Test Accuracy: 0.4078\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.05it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Train Loss: 1.5051, Train Accuracy: 0.3742, Test Loss: 1.6228, Test Accuracy: 0.3526\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.09it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50, Train Loss: 1.5013, Train Accuracy: 0.3744, Test Loss: 1.4474, Test Accuracy: 0.3993\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:54<00:00,  5.99it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50, Train Loss: 1.4641, Train Accuracy: 0.3929, Test Loss: 1.5516, Test Accuracy: 0.3845\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.15it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50, Train Loss: 1.4669, Train Accuracy: 0.3906, Test Loss: 1.4158, Test Accuracy: 0.4280\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.15it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50, Train Loss: 1.4594, Train Accuracy: 0.4054, Test Loss: 1.4565, Test Accuracy: 0.4060\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.10it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Train Loss: 1.4508, Train Accuracy: 0.3987, Test Loss: 1.4502, Test Accuracy: 0.4002\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.13it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50, Train Loss: 1.4466, Train Accuracy: 0.4087, Test Loss: 1.3931, Test Accuracy: 0.4374\n",
            "Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.10it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50, Train Loss: 1.4420, Train Accuracy: 0.4165, Test Loss: 1.4679, Test Accuracy: 0.4195\n",
            "Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.14it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50, Train Loss: 1.4481, Train Accuracy: 0.4098, Test Loss: 1.3766, Test Accuracy: 0.4495\n",
            "Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.15it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50, Train Loss: 1.4367, Train Accuracy: 0.4102, Test Loss: 1.4624, Test Accuracy: 0.4060\n",
            "Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.09it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50, Train Loss: 1.4300, Train Accuracy: 0.4160, Test Loss: 1.4791, Test Accuracy: 0.3881\n",
            "Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.13it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50, Train Loss: 1.4364, Train Accuracy: 0.4183, Test Loss: 1.4345, Test Accuracy: 0.4401\n",
            "Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.11it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50, Train Loss: 1.4197, Train Accuracy: 0.4150, Test Loss: 1.4357, Test Accuracy: 0.4275\n",
            "Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.14it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50, Train Loss: 1.4200, Train Accuracy: 0.4248, Test Loss: 1.4381, Test Accuracy: 0.4217\n",
            "Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.13it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50, Train Loss: 1.4089, Train Accuracy: 0.4275, Test Loss: 1.4597, Test Accuracy: 0.4118\n",
            "Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.10it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50, Train Loss: 1.4080, Train Accuracy: 0.4298, Test Loss: 1.4509, Test Accuracy: 0.4289\n",
            "Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.12it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50, Train Loss: 1.4127, Train Accuracy: 0.4275, Test Loss: 1.4123, Test Accuracy: 0.4258\n",
            "Epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.10it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50, Train Loss: 1.4106, Train Accuracy: 0.4331, Test Loss: 1.4069, Test Accuracy: 0.4352\n",
            "Epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.15it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50, Train Loss: 1.3992, Train Accuracy: 0.4396, Test Loss: 1.3953, Test Accuracy: 0.4352\n",
            "Epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.13it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50, Train Loss: 1.3969, Train Accuracy: 0.4390, Test Loss: 1.3954, Test Accuracy: 0.4365\n",
            "Epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.12it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50, Train Loss: 1.3931, Train Accuracy: 0.4410, Test Loss: 1.4488, Test Accuracy: 0.4307\n",
            "Epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.09it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50, Train Loss: 1.3929, Train Accuracy: 0.4377, Test Loss: 1.4450, Test Accuracy: 0.4406\n",
            "Epoch 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.15it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50, Train Loss: 1.3935, Train Accuracy: 0.4406, Test Loss: 1.4049, Test Accuracy: 0.4459\n",
            "Epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.13it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50, Train Loss: 1.3816, Train Accuracy: 0.4427, Test Loss: 1.4558, Test Accuracy: 0.4240\n",
            "Epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.14it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50, Train Loss: 1.3959, Train Accuracy: 0.4342, Test Loss: 1.4182, Test Accuracy: 0.4432\n",
            "Epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.14it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50, Train Loss: 1.3778, Train Accuracy: 0.4471, Test Loss: 1.4029, Test Accuracy: 0.4311\n",
            "Epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.12it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50, Train Loss: 1.3898, Train Accuracy: 0.4454, Test Loss: 1.3834, Test Accuracy: 0.4415\n",
            "Epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.12it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50, Train Loss: 1.3783, Train Accuracy: 0.4444, Test Loss: 1.3985, Test Accuracy: 0.4424\n",
            "Epoch 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.10it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50, Train Loss: 1.3711, Train Accuracy: 0.4469, Test Loss: 1.3921, Test Accuracy: 0.4406\n",
            "Epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.14it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50, Train Loss: 1.3744, Train Accuracy: 0.4540, Test Loss: 1.3938, Test Accuracy: 0.4397\n",
            "Epoch 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.14it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50, Train Loss: 1.3793, Train Accuracy: 0.4502, Test Loss: 1.3849, Test Accuracy: 0.4428\n",
            "Epoch 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.13it/s]\n",
            "100%|██████████| 140/140 [00:08<00:00, 17.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50, Train Loss: 1.3601, Train Accuracy: 0.4588, Test Loss: 1.3962, Test Accuracy: 0.4459\n",
            "Epoch 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:52<00:00,  6.14it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50, Train Loss: 1.3737, Train Accuracy: 0.4529, Test Loss: 1.4203, Test Accuracy: 0.4424\n",
            "Epoch 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325/325 [00:53<00:00,  6.13it/s]\n",
            "100%|██████████| 140/140 [00:07<00:00, 17.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50, Train Loss: 1.3730, Train Accuracy: 0.4538, Test Loss: 1.4306, Test Accuracy: 0.4468\n",
            "Epoch 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 202/325 [00:32<00:19,  6.17it/s]"
          ]
        }
      ],
      "source": [
        "# Train ViT\n",
        "scores = {}\n",
        "for _m in _modality:\n",
        "  train_loss, train_accuracy, test_loss, test_accuracy = train_ViT(_m)\n",
        "  scores[_m] = [train_loss, train_accuracy, test_loss, test_accuracy]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNOX2sOP0E3j"
      },
      "outputs": [],
      "source": [
        "# Print results\n",
        "print(\"\\nResults\\n---------------\\n\")\n",
        "for _m, val in scores.items():\n",
        "  print(f\"Modality: {_m}, Train Loss: {val[0]:.4f}, Train Accuracy: {val[1]:.4f}, Test Loss: {val[2]:.4f}, Test Accuracy: {val[3]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eQaDTdZ9Qak"
      },
      "outputs": [],
      "source": [
        "# Copy trained models to GDrive\n",
        "# !cp 'ViT_audio_video_fullscale_50_16_0.0001' '/content/drive/MyDrive/csci535/models/ViT'\n",
        "# !cp 'ViT_audio_fullscale_50_16_0.0001' '/content/drive/MyDrive/csci535/models/ViT'\n",
        "# !cp 'ViT_video_fullscale_50_16_0.0001' '/content/drive/MyDrive/csci535/models/ViT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5SeIAOnTI-E"
      },
      "outputs": [],
      "source": [
        "# Clear memory\n",
        "# import gc\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzFnQhXhmCiI"
      },
      "outputs": [],
      "source": [
        "# ! cd /content/drive/MyDrive/csci535/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-IcqNkP_SiI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12.2 (default)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}